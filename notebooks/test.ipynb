{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# Change the current working directory to the parent directory\n",
    "os.chdir(parent_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1391</td>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1390</td>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Autonomous Agents And Multi-Agent Systems 101:...</td>\n",
       "      <td>1. Introduction of Word2vec\\r\\n\\r\\nWord2vec is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  \\\n",
       "count                                                1391   \n",
       "unique                                               1390   \n",
       "top     Autonomous Agents And Multi-Agent Systems 101:...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                     Text  \n",
       "count                                                1391  \n",
       "unique                                               1391  \n",
       "top     1. Introduction of Word2vec\\r\\n\\r\\nWord2vec is...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/medium.csv\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vec\\r\\n\\r\\nWord2vec is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "      <td>In my last article, I introduced the concept o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "      <td>Introduction\\r\\n\\r\\nThanks to its strict imple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Data Frames as CSV Fil...</td>\n",
       "      <td>Photo credit to Mika Baumeister from Unsplash\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>Brain: A Mystery</td>\n",
       "      <td>“The most beautiful experience we can have is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>Machine Learning: Lincoln Was Ahead of His Time</td>\n",
       "      <td>Photo by Jp Valery on Unsplash\\r\\n\\r\\nIn the 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>AI and Us — an Opera Experience. In my previou...</td>\n",
       "      <td>EKHO COLLECTIVE: OPERA BEYOND SERIES\\r\\n\\r\\nIn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>Digital Skills as a Service (DSaaS)</td>\n",
       "      <td>Have you ever thought about what will be in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>Primer on The Importance of Mindful Data Colle...</td>\n",
       "      <td>Outline\\r\\n\\r\\nThere are differences in standa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1391 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  \\\n",
       "ID                                                        \n",
       "0     A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1     Hands-on Graph Neural Networks with PyTorch & ...   \n",
       "2                          How to Use ggplot2 in Python   \n",
       "3     Databricks: How to Save Data Frames as CSV Fil...   \n",
       "4     A Step-by-Step Implementation of Gradient Desc...   \n",
       "...                                                 ...   \n",
       "1386                                   Brain: A Mystery   \n",
       "1387    Machine Learning: Lincoln Was Ahead of His Time   \n",
       "1388  AI and Us — an Opera Experience. In my previou...   \n",
       "1389                Digital Skills as a Service (DSaaS)   \n",
       "1390  Primer on The Importance of Mindful Data Colle...   \n",
       "\n",
       "                                                   Text  \n",
       "ID                                                       \n",
       "0     1. Introduction of Word2vec\\r\\n\\r\\nWord2vec is...  \n",
       "1     In my last article, I introduced the concept o...  \n",
       "2     Introduction\\r\\n\\r\\nThanks to its strict imple...  \n",
       "3     Photo credit to Mika Baumeister from Unsplash\\...  \n",
       "4     A Step-by-Step Implementation of Gradient Desc...  \n",
       "...                                                 ...  \n",
       "1386  “The most beautiful experience we can have is ...  \n",
       "1387  Photo by Jp Valery on Unsplash\\r\\n\\r\\nIn the 4...  \n",
       "1388  EKHO COLLECTIVE: OPERA BEYOND SERIES\\r\\n\\r\\nIn...  \n",
       "1389  Have you ever thought about what will be in th...  \n",
       "1390  Outline\\r\\n\\r\\nThere are differences in standa...  \n",
       "\n",
       "[1391 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ID'] = range(0, len(df))\n",
    "df.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model',\n",
       " 'Text': '1. Introduction of Word2vec\\r\\n\\r\\nWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.\\r\\n\\r\\nThere are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.\\r\\n\\r\\n2. Gensim Python Library Introduction\\r\\n\\r\\nGensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.\\r\\n\\r\\nAt first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:\\r\\n\\r\\nPython >= 2.7 (tested with versions 2.7, 3.5 and 3.6)\\r\\n\\r\\n>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3\\r\\n\\r\\n>= 1.11.3 SciPy >= 0.18.1\\r\\n\\r\\n>= 0.18.1 Six >= 1.5.0\\r\\n\\r\\n>= 1.5.0 smart_open >= 1.2.1\\r\\n\\r\\nThere are two ways for installation. We could run the following code in our terminal to install genism package.\\r\\n\\r\\npip install --upgrade gensim\\r\\n\\r\\nOr, alternatively for Conda environments:\\r\\n\\r\\nconda install -c conda-forge gensim\\r\\n\\r\\n3. Implementation of word Embedding with Gensim Word2Vec Model\\r\\n\\r\\nIn this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.\\r\\n\\r\\nThis vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.\\r\\n\\r\\n>>> df = pd.read_csv(\\'data.csv\\')\\r\\n\\r\\n>>> df.head()\\r\\n\\r\\n3.1 Data Preprocessing:\\r\\n\\r\\nSince the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.\\r\\n\\r\\nGenism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.\\r\\n\\r\\nTo achieve this, we need to do the following things :\\r\\n\\r\\na. Create a new column for Make Model\\r\\n\\r\\n>>> df[\\'Maker_Model\\']= df[\\'Make\\']+ \" \" + df[\\'Model\\']\\r\\n\\r\\nb. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.\\r\\n\\r\\n# Select features from original dataset to form a new dataframe\\r\\n\\r\\n>>> df1 = df[[\\'Engine Fuel Type\\',\\'Transmission Type\\',\\'Driven_Wheels\\',\\'Market Category\\',\\'Vehicle Size\\', \\'Vehicle Style\\', \\'Maker_Model\\']] # For each row, combine all the columns into one column\\r\\n\\r\\n>>> df2 = df1.apply(lambda x: \\',\\'.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\\r\\n\\r\\n>>> df_clean = pd.DataFrame({\\'clean\\': df2}) # Create the list of list format of the custom corpus for gensim modeling\\r\\n\\r\\n>>> sent = [row.split(\\',\\') for row in df_clean[\\'clean\\']] # show the example of list of list format of the custom corpus for gensim modeling\\r\\n\\r\\n>>> sent[:2]\\r\\n\\r\\n[[\\'premium unleaded (required)\\',\\r\\n\\r\\n\\'MANUAL\\',\\r\\n\\r\\n\\'rear wheel drive\\',\\r\\n\\r\\n\\'Factory Tuner\\',\\r\\n\\r\\n\\'Luxury\\',\\r\\n\\r\\n\\'High-Performance\\',\\r\\n\\r\\n\\'Compact\\',\\r\\n\\r\\n\\'Coupe\\',\\r\\n\\r\\n\\'BMW 1 Series M\\'],\\r\\n\\r\\n[\\'premium unleaded (required)\\',\\r\\n\\r\\n\\'MANUAL\\',\\r\\n\\r\\n\\'rear wheel drive\\',\\r\\n\\r\\n\\'Luxury\\',\\r\\n\\r\\n\\'Performance\\',\\r\\n\\r\\n\\'Compact\\',\\r\\n\\r\\n\\'Convertible\\',\\r\\n\\r\\n\\'BMW 1 Series\\']]\\r\\n\\r\\n3.2. Genism word2vec Model Training\\r\\n\\r\\nWe can train the genism word2vec model with our own custom corpus as following:\\r\\n\\r\\n>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)\\r\\n\\r\\nLet’s try to understand the hyperparameters of this model.\\r\\n\\r\\nsize: The number of dimensions of the embeddings and the default is 100.\\r\\n\\r\\nwindow: The maximum distance between a target word and words around the target word. The default window is 5.\\r\\n\\r\\nmin_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\\r\\n\\r\\nworkers: The number of partitions during training and the default workers is 3.\\r\\n\\r\\nsg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\\r\\n\\r\\nAfter training the word2vec model, we can obtain the word embedding directly from the training model as following.\\r\\n\\r\\n>>> model[\\'Toyota Camry\\'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,\\r\\n\\r\\n-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,\\r\\n\\r\\n-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,\\r\\n\\r\\n0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,\\r\\n\\r\\n-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,\\r\\n\\r\\n-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,\\r\\n\\r\\n-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,\\r\\n\\r\\n0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,\\r\\n\\r\\n0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,\\r\\n\\r\\n0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],\\r\\n\\r\\ndtype=float32)\\r\\n\\r\\n4. Compute Similarities\\r\\n\\r\\nNow we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.\\r\\n\\r\\n>>> model.similarity(\\'Porsche 718 Cayman\\', \\'Nissan Van\\')\\r\\n\\r\\n0.822824584626184 >>> model.similarity(\\'Porsche 718 Cayman\\', \\'Mercedes-Benz SLK-Class\\')\\r\\n\\r\\n0.961089779453727\\r\\n\\r\\nFrom the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.\\r\\n\\r\\n>>> model1.most_similar(\\'Mercedes-Benz SLK-Class\\')[:5] [(\\'BMW M4\\', 0.9959905743598938),\\r\\n\\r\\n(\\'Maserati Coupe\\', 0.9949707984924316),\\r\\n\\r\\n(\\'Porsche Cayman\\', 0.9945154190063477),\\r\\n\\r\\n(\\'Mercedes-Benz SLS AMG GT\\', 0.9944609999656677),\\r\\n\\r\\n(\\'Maserati Spyder\\', 0.9942780137062073)]\\r\\n\\r\\nHowever, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.\\r\\n\\r\\nThe following function shows how can we generate the most similar make model based on cosine similarity.\\r\\n\\r\\ndef cosine_distance (model, word,target_list , num) :\\r\\n\\r\\ncosine_dict ={}\\r\\n\\r\\nword_list = []\\r\\n\\r\\na = model[word]\\r\\n\\r\\nfor item in target_list :\\r\\n\\r\\nif item != word :\\r\\n\\r\\nb = model [item]\\r\\n\\r\\ncos_sim = dot(a, b)/(norm(a)*norm(b))\\r\\n\\r\\ncosine_dict[item] = cos_sim\\r\\n\\r\\ndist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order\\r\\n\\r\\nfor item in dist_sort:\\r\\n\\r\\nword_list.append((item[0], item[1]))\\r\\n\\r\\nreturn word_list[0:num] # only get the unique Maker_Model\\r\\n\\r\\n>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance\\r\\n\\r\\n>>> cosine_distance (model,\\'Mercedes-Benz SLK-Class\\',Maker_Model,5) [(\\'Mercedes-Benz CLK-Class\\', 0.99737006),\\r\\n\\r\\n(\\'Aston Martin DB9\\', 0.99593246),\\r\\n\\r\\n(\\'Maserati Spyder\\', 0.99571854),\\r\\n\\r\\n(\\'Ferrari 458 Italia\\', 0.9952333),\\r\\n\\r\\n(\\'Maserati GranTurismo Convertible\\', 0.994994)]\\r\\n\\r\\n5. T-SNE Visualizations\\r\\n\\r\\nIt’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.\\r\\n\\r\\ndef display_closestwords_tsnescatterplot(model, word, size):\\r\\n\\r\\n\\r\\n\\r\\narr = np.empty((0,size), dtype=\\'f\\')\\r\\n\\r\\nword_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)\\r\\n\\r\\nfor wrd_score in close_words:\\r\\n\\r\\nwrd_vector = model[wrd_score[0]]\\r\\n\\r\\nword_labels.append(wrd_score[0])\\r\\n\\r\\narr = np.append(arr, np.array([wrd_vector]), axis=0)\\r\\n\\r\\n\\r\\n\\r\\ntsne = TSNE(n_components=2, random_state=0)\\r\\n\\r\\nnp.set_printoptions(suppress=True)\\r\\n\\r\\nY = tsne.fit_transform(arr) x_coords = Y[:, 0]\\r\\n\\r\\ny_coords = Y[:, 1]\\r\\n\\r\\nplt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):\\r\\n\\r\\nplt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\\'offset points\\')\\r\\n\\r\\nplt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\\r\\n\\r\\nplt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\\r\\n\\r\\nplt.show() >>> display_closestwords_tsnescatterplot(model, \\'Porsche 718 Cayman\\', 50)\\r\\n\\r\\nThis T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.\\r\\n\\r\\nAbout Me\\r\\n\\r\\nI am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin.',\n",
       " 'ID': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This is a sentence., This another sentence.]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in records:\n",
    "    item[\"sentences\"] = list(nlp(item[\"Text\"].replace('\\r', '').replace('\\n', '')).sents)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    item[\"sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'Missing Data and Imputation',\n",
       "  'Text': 'Missing data can skew findings, increase computational expense, and frustrate researchers. In recent years, dealing with missing data has become more prevalent in fields like biological and life sciences, as we are seeing very direct consequences of mismanaged null values¹. In response, there are more diverse methods for handling missing data emerging.\\r\\n\\r\\nThis is great for increasing the effectiveness of studies, and a bit tricky for aspiring and active data scientists keep up with. This blog post will introduce you to a few helpful concepts in dealing with missing data, and get you started with some tangible ways to clean up your data in Python that you can try out today.\\r\\n\\r\\nPhoto by Carlos Muza on Unsplash\\r\\n\\r\\nWhy do anything at all?\\r\\n\\r\\nYou may be asking yourself — why do I need to deal with missing data at all? Why not let sleeping dogs lie? Well, first of all, missing values (termed NaN, Null or NA) cause computational challenges because. Think about it — if you’re trying to sum up a column of values and find a missing one, what is 5 + NA? If we don’t know the second term in the equation, our outcome is itself NA. So we really can’t derive anything meaningful from missing values, plus it confuses most programs that expect to be handling non-empty cases.\\r\\n\\r\\nAside from this, there are three main problems that missing data causes:\\r\\n\\r\\nBias More laborious processing Reduced efficiency in outcomes\\r\\n\\r\\nThese are all pretty serious (if not just irritating) side effects of missing data, so we’ll want to find something to do with our empty cells. That’s where “imputation” comes in.\\r\\n\\r\\nImputation\\r\\n\\r\\nWebster’s Dictionary shares a “financial” definition of the term imputation, which is “the assignment of a value to something by inference from the value of the products or processes to which it contributes.” This is definitely what we want to think of here — how can we infer the value that is closest to the true value that is missing?\\r\\n\\r\\nAs an aside— it is interesting to reflect on and consider that this term is likely derived from its theological context. Here, it means “the action or process of ascribing righteousness, guilt, etc. to someone by virtue of a similar quality in another,” as in “the writings of the apostles tell us that imputation of the righteousness of Christ is given to us if we receive Christ.” Just some food for thought as we move along.\\r\\n\\r\\nMissing Data Mechanisms\\r\\n\\r\\nWhen researching imputation, you will likely find that there are different reasons for data to be missing. These reasons are given terms based on their relationship between the missing data mechanism and the missing and observed values. They help us unlock the appropriate data handling method, so they’re really helpful to have a basic understanding of. Below are 3 of the 4 most typical, and you can read more about them on “The Analysis Factor” .\\r\\n\\r\\nMissing Completely at Random (MCAR)\\r\\n\\r\\nThis one may be the easiest to think about — in this instance, data goes missing at a completely consistent rate. For example, a dataset that lacks 5% of responses from a youth survey. This is because 5% of all students were out sick the day that the survey was administered, so the values are missing at a consistent rate across the entire data set.\\r\\n\\r\\n2. Missing at Random (MAR)\\r\\n\\r\\nDespite the name similarities, MAR values are a bit more complex — and more likely to find than MCAR. These are instances that data the rate of missing data can be perfectly explained if we know another variable. For example, imagine the above dataset lacks 10% of responses from girls and 5% of responses from boys. This is because the illness spread at the school was 2x more likely to affect young women than young men. This gets more complex, and more realistic, as multiple variables influence the rate of missing values in a dataset.\\r\\n\\r\\n3. Missing Not at Random (MNAR)\\r\\n\\r\\nIn this case, the missing-ness of a certain value depends on the true value itself. This one is pretty cyclic, but I like the example given in this video of rates of missing values in a survey of library-goes that collects their names and number of un-returned library books. As the number of hoarded books increases, so does the percentage of missing values from this survey question. The problem with this one is that because the value missing is dependent on the value itself, we have a very difficult time deriving the rate it is missing.\\r\\n\\r\\nPractical Exploration and Visualization in Python\\r\\n\\r\\nWhen dealing with data in Python, Pandas is a powerful data management library to organize and manipulate datasets. It derives some of its terminology from R, and it is built on the numpy package. As such, it has some confusing aspects that are worth pointing out in relation to missing data management.\\r\\n\\r\\nThe two built-in functions, pandas.DataFrame.isna() and pandas.DataFrame.isnull() actually do exactly the same thing! Even their docs are identical. You can even confirm this in pandas’ code.\\r\\n\\r\\nThis is because pandas’ DataFrames are based on R’s DataFrames. In R na and null are two separate things. Read this post for more information. However, in python, pandas is built on top of numpy, which has neither na nor null values. Instead, numpy has NaN values (which stands for “Not a Number”). Consequently, pandas also uses NaN values².\\r\\n\\r\\nAdditionally, the Python package named missingno is a very flexible, missing data visualization tool built with matplotlib, and it works with any pandas DataFrame. Just pip install missingno to get started, and check out this Github repo to learn more.\\r\\n\\r\\nA “missingno” visualization of cyclist dataset — with Sparkline on the side\\r\\n\\r\\nAdequately visualizing your missing data is a great first step in understanding which missing data mechanism you are handling, along with the scale of missing data and hot spots to work with.\\r\\n\\r\\nMethods for Single Imputation:\\r\\n\\r\\nStarting from the simplest and moving toward more complex, below are descriptions of some of the most common ways to handle missing values and their associated pros and cons.\\r\\n\\r\\n(Note that one item or row in a dataset is referred to as an “observation.”)\\r\\n\\r\\nRow (Listwise) Deletion: Get rid of the entire observation.\\r\\n\\r\\nSimple, but can introduce a lot of bias.\\r\\n\\r\\nAn example of listwise deletion\\r\\n\\r\\n2. Mean/Median/Mode Imputation: For all observations that are non-missing, calculate the mean, median or mode of the observed values for that variable, and fill in the missing values with it. Context & spread of data are necessary pieces of information to determine which descriptor to use.\\r\\n\\r\\nOk to use if missing data is less than 3%, otherwise introduces too much bias and artificially lowers variability of data\\r\\n\\r\\n3. Hot or Cold Deck Imputation\\r\\n\\r\\n“Hot Deck Imputation” : Find all the sample subjects who are similar on other variables, then randomly choose one of their values to fill in.\\r\\n\\r\\nGood because constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive\\r\\n\\r\\n“Cold Deck Imputation” : Systematically choose the value from an individual who has similar values on other variables (e.g. the third item of each collection). This option removes randomness of hot deck imputation.\\r\\n\\r\\nPositively constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive\\r\\n\\r\\nExample of basic hot deck imputation using mean values\\r\\n\\r\\n4. Regression imputations\\r\\n\\r\\n“Regression Imputation” : Fill in with the predicted value obtained by regressing the missing variable on other variables; instead of just taking the mean, you’re taking the predicted value, based on other variables.\\r\\n\\r\\nPreserves relationships among variables involved in the imputation model, but not variability around predicted values.\\r\\n\\r\\n“Stochastic regression imputation” : The predicted value from a regression, plus a random residual value.\\r\\n\\r\\nThis has all the advantages of regression imputation but adds in the advantages of the random component.\\r\\n\\r\\nChallenges of Single Imputation\\r\\n\\r\\nThese are all great methods for handling missing values, but they do include unaccounted-for changes in standard error. Again, “The Analysis Factor” explains this trade-off perfectly below:\\r\\n\\r\\n“Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn’t know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values.”\\r\\n\\r\\nAdditionally, values found in single imputation might be biased by the specific values in the current data set, and not represent the total values of the full population. So how do we reduce the impact of these two challenges?\\r\\n\\r\\nMultiple Imputation\\r\\n\\r\\nMultiple imputation was a huge breakthrough in statistics about 20 years ago because it solved a lot of these problems with missing data (though, unfortunately not all). If done well, it leads to unbiased parameter estimates and accurate standard errors.\\r\\n\\r\\nWhile single imputation gives us a single value for the missing observation’s variable, multiple imputation gives us (you guessed it) multiple values for the missing observation’s variable and then averages them for the final value.\\r\\n\\r\\nTo get each of these averages, a multiple imputation method would run analyses with 5–10 unique samples of the dataset and run the same predictive analysis on each**. The predicted value at that point would serve as the value for that run; the data signature of these samples change each time, which causes the prediction to be a bit different. The more times you do this, the less biased the outcome will be.\\r\\n\\r\\nOnce you take the mean of these values, it is important to analyze their spread. If they’re clustering, they have a low standard deviation. If they’re not, variability is high and may be a sign that the value prediction may be less reliable.\\r\\n\\r\\nWhile this method is much more unbiased, it is also more complicated and requires more computational time and energy.\\r\\n\\r\\nConclusion\\r\\n\\r\\nIn closing, we looked at:\\r\\n\\r\\nThe importance of handing missing values in a data set The meaning (and root) of “imputation” Different reasons that data could be missing (missing data mechanisms) Ways to explore and visualize your missing data in Python Methods of single imputation An explanation of multiple imputation\\r\\n\\r\\nBut this is just a beginning! Please look into the linked resources on this post, and beyond, for further information on this topic.',\n",
       "  'ID': 234,\n",
       "  'sentences': ['Missing data can skew findings, increase computational expense, and frustrate researchers.',\n",
       "   'In recent years, dealing with missing data has become more prevalent in fields like biological and life sciences, as we are seeing very direct consequences of mismanaged null values¹. In response, there are more diverse methods for handling missing data emerging.',\n",
       "   'This is great for increasing the effectiveness of studies, and a bit tricky for aspiring and active data scientists keep up with.',\n",
       "   'This blog post will introduce you to a few helpful concepts in dealing with missing data, and get you started with some tangible ways to clean up your data in Python that you can try out today.',\n",
       "   'Photo by Carlos Muza on UnsplashWhy do anything at all?You may be asking yourself — why do I need to deal with missing data at all?',\n",
       "   'Why not let sleeping dogs lie?',\n",
       "   'Well, first of all, missing values (termed NaN, Null or NA) cause computational challenges because.',\n",
       "   'Think about it — if you’re trying to sum up a column of values and find a missing one, what is 5 + NA?',\n",
       "   'If we don’t know the second term in the equation, our outcome is itself NA.',\n",
       "   'So we really can’t derive anything meaningful from missing values, plus it confuses most programs that expect to be handling non-empty cases.',\n",
       "   'Aside from this, there are three main problems that missing data causes:Bias More laborious processing Reduced efficiency in outcomesThese are all pretty serious (if not just irritating) side effects of missing data, so we’ll want to find something to do with our empty cells.',\n",
       "   'That’s where “imputation” comes in.',\n",
       "   'ImputationWebster’s Dictionary shares a “financial” definition of the term imputation, which is “the assignment of a value to something by inference from the value of the products or processes to which it contributes.”',\n",
       "   'This is definitely what we want to think of here — how can we infer the value that is closest to the true value that is missing?As an aside— it is interesting to reflect on and consider that this term is likely derived from its theological context.',\n",
       "   'Here, it means “the action or process of ascribing righteousness, guilt, etc.',\n",
       "   'to someone by virtue of a similar quality in another,” as in “the writings of the apostles tell us that imputation of the righteousness of Christ is given to us if we receive Christ.”',\n",
       "   'Just some food for thought as we move along.',\n",
       "   'Missing Data MechanismsWhen researching imputation, you will likely find that there are different reasons for data to be missing.',\n",
       "   'These reasons are given terms based on their relationship between the missing data mechanism and the missing and observed values.',\n",
       "   'They help us unlock the appropriate data handling method, so they’re really helpful to have a basic understanding of.',\n",
       "   'Below are 3 of the 4 most typical, and you can read more about them on “The Analysis Factor” .Missing Completely at Random (MCAR)This one may be the easiest to think about — in this instance, data goes missing at a completely consistent rate.',\n",
       "   'For example, a dataset that lacks 5% of responses from a youth survey.',\n",
       "   'This is because 5% of all students were out sick the day that the survey was administered, so the values are missing at a consistent rate across the entire data set.2.',\n",
       "   'Missing at Random (MAR)Despite the name similarities, MAR values are a bit more complex — and more likely to find than MCAR.',\n",
       "   'These are instances that data the rate of missing data can be perfectly explained if we know another variable.',\n",
       "   'For example, imagine the above dataset lacks 10% of responses from girls and 5% of responses from boys.',\n",
       "   'This is because the illness spread at the school was 2x more likely to affect young women than young men.',\n",
       "   'This gets more complex, and more realistic, as multiple variables influence the rate of missing values in a dataset.3.',\n",
       "   'Missing Not at Random (MNAR)In this case, the missing-ness of a certain value depends on the true value itself.',\n",
       "   'This one is pretty cyclic, but I like the example given in this video of rates of missing values in a survey of library-goes that collects their names and number of un-returned library books.',\n",
       "   'As the number of hoarded books increases, so does the percentage of missing values from this survey question.',\n",
       "   'The problem with this one is that because the value missing is dependent on the value itself, we have a very difficult time deriving the rate it is missing.',\n",
       "   'Practical Exploration and Visualization in PythonWhen dealing with data in Python, Pandas is a powerful data management library to organize and manipulate datasets.',\n",
       "   'It derives some of its terminology from R, and it is built on the numpy package.',\n",
       "   'As such, it has some confusing aspects that are worth pointing out in relation to missing data management.',\n",
       "   'The two built-in functions, pandas.DataFrame.isna() and pandas.DataFrame.isnull() actually do exactly the same thing!',\n",
       "   'Even their docs are identical.',\n",
       "   'You can even confirm this in pandas’ code.',\n",
       "   'This is because pandas’ DataFrames are based on R’s DataFrames.',\n",
       "   'In R na and null are two separate things.',\n",
       "   'Read this post for more information.',\n",
       "   'However, in python, pandas is built on top of numpy, which has neither na nor null values.',\n",
       "   'Instead, numpy has NaN values (which stands for “Not a Number”).',\n",
       "   'Consequently, pandas also uses NaN values².Additionally, the Python package named missingno is a very flexible, missing data visualization tool built with matplotlib, and it works with any pandas DataFrame.',\n",
       "   'Just pip install missingno to get started, and check out this Github repo to learn more.',\n",
       "   'A “missingno” visualization of cyclist dataset — with Sparkline on the sideAdequately visualizing your missing data is a great first step in understanding which missing data mechanism you are handling, along with the scale of missing data and hot spots to work with.',\n",
       "   'Methods for Single Imputation:Starting from the simplest and moving toward more complex, below are descriptions of some of the most common ways to handle missing values and their associated pros and cons.(Note that one item or row in a dataset is referred to as an “observation.',\n",
       "   '”)Row (Listwise) Deletion: Get rid of the entire observation.',\n",
       "   'Simple, but can introduce a lot of bias.',\n",
       "   'An example of listwise deletion2.',\n",
       "   'Mean/Median/Mode Imputation: For all observations that are non-missing, calculate the mean, median or mode of the observed values for that variable, and fill in the missing values with it.',\n",
       "   'Context & spread of data are necessary pieces of information to determine which descriptor to use.',\n",
       "   'Ok to use if missing data is less than 3%, otherwise introduces too much bias and artificially lowers variability of data3.',\n",
       "   'Hot or Cold Deck Imputation“Hot Deck Imputation” : Find all the sample subjects who are similar on other variables, then randomly choose one of their values to fill in.',\n",
       "   'Good because constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive“Cold Deck Imputation” : Systematically choose the value from an individual who has similar values on other variables (e.g. the third item of each collection).',\n",
       "   'This option removes randomness of hot deck imputation.',\n",
       "   'Positively constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensiveExample of basic hot deck imputation using mean values4.',\n",
       "   'Regression imputations“Regression Imputation” : Fill in with the predicted value obtained by regressing the missing variable on other variables; instead of just taking the mean, you’re taking the predicted value, based on other variables.',\n",
       "   'Preserves relationships among variables involved in the imputation model, but not variability around predicted values.',\n",
       "   '“Stochastic regression imputation” : The predicted value from a regression, plus a random residual value.',\n",
       "   'This has all the advantages of regression imputation but adds in the advantages of the random component.',\n",
       "   'Challenges of Single ImputationThese are all great methods for handling missing values, but they do include unaccounted-for changes in standard error.',\n",
       "   'Again, “The Analysis Factor” explains this trade-off perfectly below:“Since the imputed observations are themselves estimates, their values have corresponding random error.',\n",
       "   'But when you put in that estimate as a data point, your software doesn’t know that.',\n",
       "   'So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values.',\n",
       "   '”Additionally, values found in single imputation might be biased by the specific values in the current data set, and not represent the total values of the full population.',\n",
       "   'So how do we reduce the impact of these two challenges?Multiple ImputationMultiple imputation was a huge breakthrough in statistics about 20 years ago because it solved a lot of these problems with missing data (though, unfortunately not all).',\n",
       "   'If done well, it leads to unbiased parameter estimates and accurate standard errors.',\n",
       "   'While single imputation gives us a single value for the missing observation’s variable, multiple imputation gives us (you guessed it) multiple values for the missing observation’s variable and then averages them for the final value.',\n",
       "   'To get each of these averages, a multiple imputation method would run analyses with 5–10 unique samples of the dataset and run the same predictive analysis on each**.',\n",
       "   'The predicted value at that point would serve as the value for that run; the data signature of these samples change each time, which causes the prediction to be a bit different.',\n",
       "   'The more times you do this, the less biased the outcome will be.',\n",
       "   'Once you take the mean of these values, it is important to analyze their spread.',\n",
       "   'If they’re clustering, they have a low standard deviation.',\n",
       "   'If they’re not, variability is high and may be a sign that the value prediction may be less reliable.',\n",
       "   'While this method is much more unbiased, it is also more complicated and requires more computational time and energy.',\n",
       "   'ConclusionIn closing, we looked at:The importance of handing missing values in a data set The meaning (and root) of “imputation” Different reasons that data could be missing (missing data mechanisms) Ways to explore and visualize your missing data in Python Methods of single imputation An explanation of multiple imputationBut this is just a beginning!',\n",
       "   'Please look into the linked resources on this post, and beyond, for further information on this topic.'],\n",
       "  'sentence_count_spacy': 78}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(records, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split size to turn groups of sentences into chunks\n",
    "num_sentence_chunk_size = 5 \n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in records:\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'Which U.S. States Have the Most Neighbors?',\n",
       "  'Text': 'Photo by Joey Csunyo on Unsplash\\r\\n\\r\\nThis is going to be a pretty quick and dirty post on using python to determine whether one U.S. state (or any arbitrary geography) borders another U.S. state (or any other arbitrary geography).\\r\\n\\r\\nOur input will be a GeoJSON (just a JSON describing a complex shape) of U.S. States, which you can get from my GitHub here. And, our output will be a dictionary in python which maps each U.S. state to a single number indicating how many neighboring states it has.\\r\\n\\r\\nOur main tool for this post will be the python library shapely which helps us manipulate complex geographies in python.\\r\\n\\r\\nThe procedure will be pretty straightforward: for each U.S. state, we can loop over every other U.S. state and then check whether or not the two states touch. If they do, we can update a running list of neighboring states for the current state in question.\\r\\n\\r\\nFirst we will need to convert the GeoJSON into objects that shapely can understand. The two main shapely objects that we’ll be using are:\\r\\n\\r\\nPolygon objects: basically a single bounded shape\\r\\n\\r\\nobjects: basically a single bounded shape MultiPolygon objects: basically a collection of Polygon objects (because some U.S. states consist of multiple geographic pieces)\\r\\n\\r\\nHere’s the code to parse the GeoJSON:',\n",
       "  'ID': 886,\n",
       "  'sentences': ['Photo by Joey Csunyo on UnsplashThis is going to be a pretty quick and dirty post on using python to determine whether one U.S. state (or any arbitrary geography) borders another U.S. state (or any other arbitrary geography).Our input will be a GeoJSON (just a JSON describing a complex shape) of U.S. States, which you can get from my GitHub here.',\n",
       "   'And, our output will be a dictionary in python which maps each U.S. state to a single number indicating how many neighboring states it has.',\n",
       "   'Our main tool for this post will be the python library shapely which helps us manipulate complex geographies in python.',\n",
       "   'The procedure will be pretty straightforward: for each U.S. state, we can loop over every other U.S. state and then check whether or not the two states touch.',\n",
       "   'If they do, we can update a running list of neighboring states for the current state in question.',\n",
       "   'First we will need to convert the GeoJSON into objects that shapely can understand.',\n",
       "   'The two main shapely objects that we’ll be using are:Polygon objects: basically a single bounded shapeobjects: basically a single bounded shape MultiPolygon objects: basically a collection of Polygon objects (because some U.S. states consist of multiple geographic pieces)Here’s the code to parse the GeoJSON:'],\n",
       "  'sentence_count_spacy': 7,\n",
       "  'sentence_chunks': [['Photo by Joey Csunyo on UnsplashThis is going to be a pretty quick and dirty post on using python to determine whether one U.S. state (or any arbitrary geography) borders another U.S. state (or any other arbitrary geography).Our input will be a GeoJSON (just a JSON describing a complex shape) of U.S. States, which you can get from my GitHub here.',\n",
       "    'And, our output will be a dictionary in python which maps each U.S. state to a single number indicating how many neighboring states it has.',\n",
       "    'Our main tool for this post will be the python library shapely which helps us manipulate complex geographies in python.',\n",
       "    'The procedure will be pretty straightforward: for each U.S. state, we can loop over every other U.S. state and then check whether or not the two states touch.',\n",
       "    'If they do, we can update a running list of neighboring states for the current state in question.'],\n",
       "   ['First we will need to convert the GeoJSON into objects that shapely can understand.',\n",
       "    'The two main shapely objects that we’ll be using are:Polygon objects: basically a single bounded shapeobjects: basically a single bounded shape MultiPolygon objects: basically a collection of Polygon objects (because some U.S. states consist of multiple geographic pieces)Here’s the code to parse the GeoJSON:']],\n",
       "  'num_chunks': 2}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(records, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11856"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Split each chunk into its own item\n",
    "record_chunks = []\n",
    "for item in records:\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"ID\"] = item[\"ID\"]\n",
    "        chunk_dict[\"Title\"] = item[\"Title\"]\n",
    "        \n",
    "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        # Get stats about the chunk\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        record_chunks.append(chunk_dict)\n",
    "\n",
    "# How many chunks do we have?\n",
    "len(record_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 1017,\n",
       "  'Title': 'Car Image Classification Using Features Extracted from Pre-trained Neural Networks',\n",
       "  'sentence_chunk': 'Likewise, as seen in figure 7, Sedans that are misclassified as Convertibles/Coupes are more colorful than the Sedans that are correctly classified. This is an encouraging result in most cases as this will pick up colors specific to certain car Make/Model but the weight placed on color is higher than the weight placed on features that represent shape and size. ConclusionsThe primary conclusions from the above image classification analysis are:Prototyping a classification model using pretrained CNN features is quite effective and easier than fully building a deep neural network from scratch. Error analysis is quite useful, and provides insights on how models can be employed. ReferencesGitHub Repo:Feature Extraction: extract_features.pyModel Building: 3_FinalModelsRuns.ipynbError Analysis: 3_Results_Presentation.ipynb',\n",
       "  'chunk_char_count': 827,\n",
       "  'chunk_word_count': 113,\n",
       "  'chunk_token_count': 206.75}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(record_chunks, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11856.00</td>\n",
       "      <td>11856.00</td>\n",
       "      <td>11856.00</td>\n",
       "      <td>11856.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>697.86</td>\n",
       "      <td>645.16</td>\n",
       "      <td>102.92</td>\n",
       "      <td>161.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>401.69</td>\n",
       "      <td>324.12</td>\n",
       "      <td>45.60</td>\n",
       "      <td>81.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>345.00</td>\n",
       "      <td>460.00</td>\n",
       "      <td>76.00</td>\n",
       "      <td>115.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>714.00</td>\n",
       "      <td>604.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>151.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1054.00</td>\n",
       "      <td>773.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>193.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1390.00</td>\n",
       "      <td>5621.00</td>\n",
       "      <td>577.00</td>\n",
       "      <td>1405.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  chunk_char_count  chunk_word_count  chunk_token_count\n",
       "count  11856.00          11856.00          11856.00           11856.00\n",
       "mean     697.86            645.16            102.92             161.29\n",
       "std      401.69            324.12             45.60              81.03\n",
       "min        0.00              2.00              1.00               0.50\n",
       "25%      345.00            460.00             76.00             115.00\n",
       "50%      714.00            604.00             99.00             151.00\n",
       "75%     1054.00            773.00            125.00             193.25\n",
       "max     1390.00           5621.00            577.00            1405.25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(record_chunks)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk token count: 16.25 | Text: The encoding and decoding process all happen within the data set.\n",
      "Chunk token count: 20.5 | Text: Ralph JohnsonHere are a few all-time classics you should strive to read this year:\n",
      "Chunk token count: 7.75 | Text: So why do I feel so much worse?\n",
      "Chunk token count: 21.25 | Text: Checking original test imagesCurious about the images that I’ve picked?Here they are:\n",
      "Chunk token count: 17.75 | Text: Lab. Syst.,1998, 44, 175 — to maximize the explained covariance on the…\n"
     ]
    }
   ],
   "source": [
    "# Show random chunks with under 30 tokens in length\n",
    "min_token_length = 30\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Agata\\.conda\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Sentences Transformers library provides an easy and open-source way to create embeddings.\n",
      "Embedding: [-2.07981374e-02  3.03164907e-02 -2.01217979e-02  6.86483532e-02\n",
      " -2.55255420e-02 -8.47688597e-03 -2.07085031e-04 -6.32377118e-02\n",
      "  2.81606112e-02 -3.33353020e-02  3.02634854e-02  5.30720577e-02\n",
      " -5.03526479e-02  2.62288060e-02  3.33313867e-02 -4.51578647e-02\n",
      "  3.63044403e-02 -1.37114024e-03 -1.20171197e-02  1.14946719e-02\n",
      "  5.04510701e-02  4.70856950e-02  2.11912710e-02  5.14607392e-02\n",
      " -2.03746427e-02 -3.58889289e-02 -6.67863991e-04 -2.94393171e-02\n",
      "  4.95858900e-02 -1.05639501e-02 -1.52013879e-02 -1.31752633e-03\n",
      "  4.48197015e-02  1.56022999e-02  8.60379885e-07 -1.21391134e-03\n",
      " -2.37978678e-02 -9.09396447e-04  7.34481821e-03 -2.53929826e-03\n",
      "  5.23369834e-02 -4.68043387e-02  1.66214649e-02  4.71579060e-02\n",
      " -4.15599607e-02  9.01933818e-04  3.60278711e-02  3.42214555e-02\n",
      "  9.68227163e-02  5.94828948e-02 -1.64984651e-02 -3.51249278e-02\n",
      "  5.92518551e-03 -7.07988336e-04 -2.41031423e-02  3.49740945e-02\n",
      " -2.94746831e-02  6.04270399e-03 -9.80648957e-03  2.83217710e-02\n",
      " -1.85376201e-02  3.63213308e-02  1.30292634e-02 -3.71233411e-02\n",
      "  5.27256653e-02 -1.19706681e-02 -7.18082413e-02  1.24431411e-02\n",
      " -6.70565106e-03  7.42155015e-02  1.16357403e-02 -1.74533147e-02\n",
      " -1.82405785e-02 -1.88930370e-02  2.82414686e-02  1.32829091e-02\n",
      " -3.51909883e-02  8.87361006e-04  5.79572357e-02  3.22092995e-02\n",
      " -3.48586403e-03  4.13768478e-02  1.44357821e-02 -3.28044407e-02\n",
      " -9.79081076e-03 -3.16492654e-02  4.23871092e-02 -4.70846854e-02\n",
      " -2.08937358e-02 -1.91249233e-02 -1.22626787e-02  1.01605067e-02\n",
      "  3.91922146e-02 -2.61895768e-02  1.09028202e-02  1.35722822e-02\n",
      " -5.79267144e-02 -3.21500041e-02 -5.75726759e-03 -2.43516080e-02\n",
      "  5.23417480e-02  5.46127511e-03 -2.30995808e-02  2.57172040e-03\n",
      " -6.63345456e-02  3.54126021e-02 -1.03907268e-02  2.25409828e-02\n",
      " -1.84574239e-02 -2.42005941e-02 -4.78364713e-02 -4.79234848e-03\n",
      " -5.34138381e-02  3.01790554e-02 -1.56130502e-02 -5.51476032e-02\n",
      " -3.91874351e-02  5.92152700e-02 -3.47646810e-02  9.68119875e-03\n",
      "  2.13415362e-02  2.30417401e-02  1.91712305e-02  2.77378540e-02\n",
      " -7.73508009e-03  1.04445787e-02 -2.67719887e-02 -2.40199510e-02\n",
      " -1.92289501e-02  3.91500909e-03 -2.54714619e-02  3.61942835e-02\n",
      "  5.12866974e-02 -8.41697212e-03 -3.13829854e-02  1.47484383e-02\n",
      "  2.13939641e-02 -3.84901240e-02  2.01945920e-02  1.20765865e-02\n",
      " -3.12085077e-03  7.84031302e-03  3.30336811e-03 -4.94357608e-02\n",
      "  5.83886616e-02  3.26129212e-03  4.84489370e-03 -4.50682230e-02\n",
      "  2.45682821e-02  3.55427936e-02 -5.32505773e-02  9.21152681e-02\n",
      "  2.04395279e-02 -3.36951800e-02 -6.19804002e-02 -2.11038999e-02\n",
      "  7.82359391e-02  5.11908084e-02  5.93171157e-02 -1.25084727e-04\n",
      "  4.96349446e-02 -1.55722694e-02 -3.35675618e-03  1.82016175e-02\n",
      " -2.73444317e-02 -1.08771957e-02  1.41476011e-02  1.09876804e-02\n",
      "  4.32550441e-03  8.23311731e-02 -9.85372579e-04  7.58790895e-02\n",
      "  9.44994669e-03  2.37687789e-02  1.61928833e-02  6.24993779e-02\n",
      "  4.75922078e-02 -3.92627763e-03  9.07524601e-02  4.49873954e-02\n",
      " -3.47131044e-02  2.14077346e-02 -3.35604846e-02  4.93849367e-02\n",
      "  1.08670173e-02  2.63447221e-02 -3.26089300e-02  8.00303742e-02\n",
      "  9.29768384e-03  7.16573838e-03 -2.79172286e-02 -3.06820553e-02\n",
      "  4.01063636e-03 -4.93906699e-02 -3.13776778e-03  4.00537625e-02\n",
      " -3.97855043e-02  5.48014231e-02  1.36060080e-05 -8.38373527e-02\n",
      " -1.21547459e-02  3.40950228e-02  3.22400755e-03  6.11846372e-02\n",
      "  5.60066961e-02  9.62878950e-03  2.54615806e-02 -4.64168936e-02\n",
      " -3.98899950e-02  7.68132582e-02  2.28408668e-02 -2.26567518e-02\n",
      " -1.91192776e-02 -6.53027967e-02  4.56780754e-02 -4.43655113e-03\n",
      "  1.49631966e-02 -2.15078481e-02  2.74244859e-03  1.90358814e-02\n",
      "  5.91888279e-02 -2.47569531e-02  3.66144776e-02  5.63083924e-02\n",
      " -8.86443164e-03 -1.74324643e-02 -1.03286328e-03  2.47667115e-02\n",
      "  1.30763007e-02  5.04632890e-02 -5.28498692e-03  5.92396930e-02\n",
      "  6.29906505e-02 -4.36783507e-02 -4.97831069e-02  5.56297190e-02\n",
      " -2.44854074e-02 -8.26755688e-02  2.04910934e-02 -1.06446370e-01\n",
      "  6.64837938e-03  2.97303721e-02 -2.36440524e-02 -8.84610042e-03\n",
      "  2.45556352e-03 -3.35234776e-02  7.52212629e-02 -5.89879863e-02\n",
      " -3.67807895e-02  3.41542326e-02  5.41130640e-02 -1.74904857e-02\n",
      "  1.33920033e-02  4.71681803e-02  1.46117071e-02 -2.12310720e-02\n",
      " -6.55338839e-02  1.23857530e-02  2.76074745e-02 -8.02159403e-03\n",
      " -4.59636524e-02 -8.22445750e-03  9.16950684e-03 -1.56398639e-02\n",
      "  7.54618039e-03  1.58314325e-03 -3.03958766e-02 -5.10670803e-02\n",
      "  1.96313746e-02  1.26263024e-02 -1.51734159e-03  2.02890914e-02\n",
      "  1.37817813e-02  1.49110341e-02  2.50766538e-02 -3.62870507e-02\n",
      "  1.08084455e-02  2.74135894e-03  1.81510486e-02  5.39872386e-02\n",
      " -4.74541821e-02 -4.28731032e-02 -2.89914627e-02  2.13234816e-02\n",
      " -3.85161377e-02  6.31921813e-02 -5.77975735e-02  3.77891958e-03\n",
      " -2.54394468e-02 -1.77214300e-04  9.08247381e-03  1.59095209e-02\n",
      "  4.11799289e-02 -3.94369215e-02 -9.64432955e-03  1.30792344e-02\n",
      "  6.87962323e-02  4.32193242e-02  7.54075591e-04  6.77741542e-02\n",
      "  4.93706390e-02 -3.47819156e-03 -1.06054898e-02  6.72492897e-03\n",
      " -1.39062032e-02  4.88276593e-02 -1.05736041e-02  3.50225414e-03\n",
      "  2.90225144e-03  2.40044519e-02  1.20272441e-02 -2.09796745e-02\n",
      " -2.39112545e-02  3.26579399e-02 -1.01326033e-03 -5.92754781e-03\n",
      " -7.40533695e-03  3.63139925e-03 -2.26698779e-02 -2.21242048e-02\n",
      "  3.86995859e-02  1.72322057e-02  3.85921523e-02 -5.04710972e-02\n",
      " -3.42144966e-02 -4.00443077e-02 -3.57910953e-02 -4.62560467e-02\n",
      "  6.70231655e-02 -4.61647753e-03 -3.29680531e-03  2.08444204e-02\n",
      " -5.14258444e-03 -5.00850454e-02  2.22504605e-02  4.66933362e-02\n",
      "  1.36208460e-02  1.77530665e-02  4.28080652e-03 -2.79332437e-02\n",
      " -1.93421133e-02 -3.87860648e-02 -3.09555326e-02 -6.64134175e-02\n",
      " -1.13434196e-02  1.64267030e-02  1.77629907e-02 -2.28224974e-03\n",
      " -3.30088325e-02 -1.36267347e-03 -2.17934698e-02 -2.67508700e-02\n",
      " -1.26376050e-02  1.61864539e-03 -4.95672747e-02  7.85445198e-02\n",
      "  4.10962291e-02  9.65922233e-03 -1.14643043e-02  1.68853346e-03\n",
      "  5.37663624e-02  2.05538399e-03 -4.11201082e-02  1.46330390e-02\n",
      " -3.75563987e-02 -3.35689113e-02  5.19258482e-03 -6.33088946e-02\n",
      "  3.32963504e-02  8.76123831e-03  1.33859995e-03 -3.95751651e-03\n",
      " -1.61678065e-02  8.26747119e-02  4.75944355e-02 -3.43055278e-02\n",
      "  2.50881687e-02 -3.50976065e-02  3.68657671e-02  4.12652828e-03\n",
      "  4.16018255e-02 -1.35181785e-01 -4.76337522e-02 -1.20025184e-02\n",
      " -3.48891839e-02  3.25454548e-02 -2.93573225e-03 -4.85056033e-03\n",
      " -1.04223765e-01  2.78610773e-02  1.41570196e-02  3.94396186e-02\n",
      " -3.88806015e-02 -1.42463492e-02 -5.19984476e-02  8.92735366e-03\n",
      " -1.99771114e-02 -2.51724925e-02 -3.41300406e-02  1.93041693e-02\n",
      " -5.20207398e-02 -6.71999827e-02 -9.46363620e-03 -1.25589361e-03\n",
      " -5.66048585e-02  2.62097921e-02  9.91586130e-03  4.38286662e-02\n",
      "  2.26635975e-03 -3.11896391e-02 -6.25468418e-02 -3.87792438e-02\n",
      " -6.83938861e-02  4.93722782e-02  5.85508123e-02 -4.08730097e-02\n",
      " -1.98638588e-02 -2.12635081e-02  4.98037040e-02 -4.51747850e-02\n",
      " -2.37141531e-02  2.32675038e-02  1.00594781e-01  9.87112336e-03\n",
      " -1.38014583e-02 -5.21041006e-02  9.08214133e-03  1.72427949e-02\n",
      "  5.91432601e-02  2.62337122e-02 -7.04642851e-03 -1.50031904e-02\n",
      " -3.76661448e-03  6.28265925e-03 -5.23982234e-02 -4.96638864e-02\n",
      "  3.06610968e-02 -3.33646522e-03  2.34911572e-02 -8.58830139e-02\n",
      " -4.62449864e-02  5.59701324e-02  3.09090072e-04  2.01728940e-02\n",
      " -2.98069371e-03  1.76645219e-02  1.54669462e-02 -7.41718262e-02\n",
      "  7.34988600e-03 -1.05014658e-02  2.45247576e-02  1.36878472e-02\n",
      " -1.17803505e-02  4.51543964e-02  3.29039581e-02 -3.50394403e-03\n",
      " -2.71315556e-02 -5.27365096e-02 -4.60164696e-02  2.22850200e-02\n",
      "  2.62271799e-02  5.56150358e-03  1.45788966e-02 -2.97145154e-02\n",
      "  3.57042328e-02  2.22534128e-02  3.89617719e-02 -7.92634487e-02\n",
      " -9.01089050e-03  2.19012927e-02 -5.49065880e-03  8.69960897e-03\n",
      "  4.33030874e-02 -2.12631822e-02  1.13291861e-02 -6.33699298e-02\n",
      "  3.63723226e-02  2.67442688e-02 -6.64251372e-02  1.70499627e-02\n",
      " -2.79691815e-02  2.36358983e-03 -1.81953367e-02  1.52955521e-02\n",
      " -8.50431062e-03  1.16647966e-02 -9.75921750e-02 -2.92093586e-02\n",
      " -5.42547256e-02  3.61234732e-02  3.25116180e-02  8.26972257e-03\n",
      " -2.96558253e-04  1.11556631e-02 -3.85188572e-02  2.36161388e-02\n",
      "  9.85919591e-03  5.73998280e-02  4.86060679e-02 -1.37580102e-02\n",
      " -6.19215379e-03  1.11972773e-02 -3.37175131e-02 -1.10515282e-02\n",
      " -7.08333179e-02 -1.01816272e-02 -3.66010591e-02 -1.55561250e-02\n",
      " -2.13109795e-02 -1.02760568e-02 -4.35733795e-02  5.55186458e-02\n",
      " -3.76548022e-02  5.29251769e-02 -3.45223881e-02 -2.43008556e-03\n",
      "  7.25553259e-02  4.45066160e-03  4.71416600e-02 -9.43880528e-03\n",
      " -1.98978204e-02  5.71899526e-02  8.60539973e-02 -5.25057800e-02\n",
      " -1.39550259e-02  1.17373439e-02  1.33974059e-02 -4.73052375e-02\n",
      " -5.41673750e-02  4.62725870e-02 -2.58970540e-02  1.51415970e-02\n",
      "  3.38944793e-02 -3.78253008e-03 -5.76042868e-02 -1.60082243e-02\n",
      "  2.42738165e-02  3.37360352e-02 -1.96820851e-02 -2.53464412e-02\n",
      " -4.75616902e-02 -5.68755791e-02 -2.28193998e-02  3.83186750e-02\n",
      " -1.78330913e-02  1.35963513e-02  7.85975775e-04  9.74011701e-03\n",
      "  3.34298462e-02 -2.60134265e-02 -7.38573726e-03  3.56451124e-02\n",
      " -2.68532317e-02 -7.53623918e-02 -2.66984161e-02 -4.46457677e-33\n",
      " -3.31645906e-02  1.41704241e-02 -3.92909534e-02 -3.46318632e-02\n",
      " -5.88680804e-03 -1.18212467e-02  1.53951440e-02  1.18474131e-02\n",
      "  1.07757524e-02  3.62141393e-02  7.87951238e-03 -2.31844522e-02\n",
      "  1.07623264e-02  1.72346085e-02  9.54224204e-04  2.83640623e-02\n",
      "  2.37420145e-02 -1.48057370e-02  1.24193938e-03  3.52352904e-03\n",
      "  2.33735181e-02  5.58307879e-02  5.38328588e-02 -3.74078415e-02\n",
      " -2.11805031e-02  1.52740919e-04 -7.27787474e-03  5.50560839e-03\n",
      "  3.05824336e-02  4.54633571e-02 -3.35786678e-02  3.16142328e-02\n",
      " -2.56392569e-03  3.96354981e-02 -1.47573277e-02  5.67167290e-02\n",
      " -5.62787764e-02 -5.04603609e-03  3.56154814e-02 -2.76198555e-02\n",
      " -2.32292283e-02 -4.63292859e-02 -3.70919257e-02 -4.23189476e-02\n",
      "  3.70305888e-02  7.88715109e-03  3.85176353e-02  1.74771936e-03\n",
      "  5.62763447e-03  6.18103333e-03 -6.90268725e-02 -9.42969322e-03\n",
      " -7.74679892e-03  1.68349929e-02  1.22766811e-02  2.26406939e-02\n",
      "  1.21009294e-02  1.11743743e-02  1.21538751e-02 -1.16861556e-02\n",
      " -4.41612564e-02  2.30048206e-02  2.20671631e-02 -5.87504841e-02\n",
      " -3.96428779e-02  6.83135092e-02 -3.29947919e-02 -3.66774127e-02\n",
      " -3.53654958e-02  1.76184978e-02  6.95642550e-03  5.92692420e-02\n",
      "  4.12156284e-02  7.98109844e-02 -5.36562083e-03  1.14238253e-02\n",
      " -2.96388585e-02 -1.15411123e-02  2.22811457e-02  7.93188531e-03\n",
      "  2.60356274e-02  1.28211668e-02  1.71345752e-02 -6.90193195e-03\n",
      " -1.07603790e-02  1.35715278e-02 -9.90727101e-04 -6.16075248e-02\n",
      "  4.40513454e-02 -8.26604315e-04 -2.78341006e-02 -1.23619596e-02\n",
      "  1.34629440e-02 -3.85745354e-02  1.08700036e-03  2.18713153e-02\n",
      " -3.32398564e-02  1.84615478e-02 -5.10105258e-03  3.74665074e-02\n",
      " -3.67544894e-03 -2.19246168e-02 -4.96484293e-03 -9.59822163e-03\n",
      "  2.33591236e-02  1.04876375e-02  4.38722074e-02 -1.51424306e-02\n",
      " -6.30309880e-02  8.23257025e-03 -1.09130768e-02 -4.06409465e-02\n",
      " -6.21690452e-02  2.21325997e-02 -2.71433759e-02  4.05539535e-02\n",
      " -8.09452683e-03 -1.76404684e-03  3.01525835e-02 -5.42262942e-03\n",
      " -4.69821654e-02 -1.73768029e-02  4.11631241e-02  3.20636071e-02\n",
      " -2.22943798e-02 -1.58161912e-02 -4.50720899e-02  5.69486730e-02\n",
      "  4.71596420e-02 -5.78058958e-02  1.32474797e-02 -4.71289828e-03\n",
      "  1.66824520e-07  4.81090657e-02  5.03628403e-02  5.45263924e-02\n",
      "  2.07568556e-02 -1.19081121e-02 -6.37492444e-03  5.26369829e-03\n",
      "  7.21948743e-02 -2.21763123e-02  2.20103506e-02 -9.90454806e-04\n",
      " -1.37163475e-02  6.89210137e-03  2.46912148e-02 -1.39462024e-01\n",
      "  2.56979070e-03 -4.64827903e-02 -4.04967293e-02 -6.08555898e-02\n",
      " -1.53213162e-02  1.36129886e-01  9.45034549e-02  4.25741524e-02\n",
      "  4.67131063e-02 -2.30677687e-02 -1.20965810e-02  3.86673100e-02\n",
      "  2.11651786e-03 -2.51473058e-02 -1.15076434e-02 -3.46506387e-02\n",
      " -2.29533967e-02 -6.33848645e-03 -3.05175912e-02 -1.56236598e-02\n",
      "  1.39513863e-02  3.27476533e-04  2.00332631e-03  4.15108912e-03\n",
      " -2.22924352e-02 -3.62589844e-02 -2.36579031e-02 -1.87817831e-02\n",
      " -1.96288899e-02  4.52126116e-02 -8.12569261e-02 -2.14569196e-02\n",
      " -4.41543013e-02 -2.68476289e-02  2.01974157e-02  2.82993843e-03\n",
      " -1.95011441e-02 -3.45331468e-02  2.26913840e-02  3.78325544e-02\n",
      " -1.02544092e-02 -2.19750963e-03 -8.96744207e-02 -4.50031199e-02\n",
      "  8.09703395e-03 -2.05805395e-02 -2.02998202e-02 -2.09922288e-02\n",
      " -1.79405101e-02  5.81897013e-02 -7.63656246e-03  1.50847193e-02\n",
      "  1.78279755e-34  4.86179367e-02  4.22228612e-02  4.71596532e-02\n",
      "  5.89047298e-02  3.99784781e-02 -5.27070872e-02  1.56905930e-02\n",
      " -5.25149459e-04  1.13651948e-02 -6.56410679e-02 -2.20849067e-02]\n",
      "\n",
      "Sentence: Sentences can be embedded one by one or as a list of strings.\n",
      "Embedding: [ 4.31717783e-02 -5.38701080e-02 -3.78044546e-02  4.27235663e-02\n",
      " -2.35409308e-02  3.44861299e-02  2.89586950e-02  1.92816474e-03\n",
      "  2.41732784e-02 -3.17012183e-02  7.32856095e-02  1.25589622e-02\n",
      "  3.64620872e-02 -2.05251873e-02  2.81973835e-02 -6.87329695e-02\n",
      "  4.22230959e-02  9.31733462e-04  3.54035385e-02  1.41787073e-02\n",
      "  7.83994980e-03  2.31179502e-02 -4.84741479e-03  1.07174013e-02\n",
      "  4.39492241e-03  5.47802541e-03 -3.80338617e-02 -3.05488054e-03\n",
      "  5.72233507e-03 -6.78213909e-02 -4.88007665e-02 -1.45032341e-02\n",
      "  6.68007089e-03 -7.17479587e-02  1.64644905e-06  1.07564051e-02\n",
      " -3.60922143e-02 -2.37057284e-02 -5.22791892e-02  3.46110836e-02\n",
      " -5.42173814e-03  1.62611287e-02  1.96564738e-02  2.25395542e-02\n",
      " -2.25997320e-03  4.06342521e-02  8.17157924e-02  2.48179249e-02\n",
      "  5.31883985e-02  7.82715231e-02 -1.91813298e-02 -1.94087271e-02\n",
      " -2.62805466e-02 -2.44083814e-02  5.49406037e-02  1.90319140e-02\n",
      "  1.60811115e-02 -2.68895850e-02 -8.24691635e-03  7.33443797e-02\n",
      "  1.00123007e-02  2.93315537e-02  3.42867710e-03 -2.13270038e-02\n",
      " -1.62446243e-03 -5.56256482e-03 -7.64880180e-02 -5.85450120e-02\n",
      " -2.82272231e-02  7.51850475e-03  7.11225942e-02  1.95452268e-03\n",
      "  5.45926765e-03  3.22313234e-03  5.12800068e-02 -3.54105905e-02\n",
      " -5.03608882e-02  4.70519513e-02  5.15473215e-03  1.52287381e-02\n",
      " -1.06680533e-02  3.16298790e-02 -9.09037329e-03 -4.01326977e-02\n",
      " -4.35236059e-02 -1.94969457e-02  1.65605005e-02 -4.71168682e-02\n",
      " -3.92091721e-02 -3.07756588e-02 -2.94167474e-02 -4.20826524e-02\n",
      "  2.27073883e-03 -2.78329514e-02  1.69421732e-02  7.74501171e-03\n",
      " -5.23741618e-02 -4.50039431e-02  3.83605845e-02 -4.90786918e-02\n",
      "  5.06618991e-02  1.01615097e-02 -1.25022149e-02 -4.64553852e-03\n",
      " -1.54539850e-02  1.58862174e-02  1.18369507e-02 -3.59232575e-02\n",
      " -7.76226148e-02  3.43358852e-02 -2.14709640e-02 -6.86098635e-02\n",
      " -5.46235889e-02  7.83901513e-02 -3.00703067e-02 -3.37550044e-02\n",
      " -4.04998995e-02  4.80514914e-02  9.53902956e-03  2.31399108e-02\n",
      " -8.16115513e-02 -6.51690876e-03  1.54213011e-02  7.04257190e-02\n",
      " -1.25069031e-02 -2.48266477e-02 -1.71329062e-02  6.13110792e-03\n",
      "  5.44412807e-02 -1.40566267e-02 -6.24516467e-03  3.65787521e-02\n",
      "  7.36230165e-02 -6.05684659e-03 -3.61629911e-02 -1.42205425e-03\n",
      "  4.43165638e-02 -3.14517040e-03  3.18767838e-02 -1.30947921e-02\n",
      " -3.69525142e-02 -4.98030102e-03  1.30016857e-03 -2.05213577e-02\n",
      "  2.06277464e-02  5.93869528e-03 -3.07159056e-03 -3.97512801e-02\n",
      "  4.29889821e-02  6.49802238e-02 -6.76022395e-02  5.41655235e-02\n",
      "  1.52564794e-03 -3.72908600e-02 -4.02427278e-02 -2.28771642e-02\n",
      "  1.31769791e-01  4.87873564e-03  1.39470911e-02  4.92436066e-02\n",
      "  2.49219649e-02 -8.76091793e-03 -5.38769085e-03 -2.65595336e-02\n",
      " -1.19766602e-02 -2.32006665e-02 -2.67433953e-02  5.66914259e-03\n",
      "  2.21721530e-02  4.67293970e-02 -5.78486919e-02  8.22120458e-02\n",
      " -3.36836767e-03  8.09647217e-02  1.41423428e-02  1.02393106e-01\n",
      " -5.76836336e-03 -1.15876868e-02  4.90584895e-02  5.87829947e-02\n",
      "  6.50030449e-02  4.74621989e-02 -2.89463978e-02 -1.76581286e-03\n",
      "  3.32561210e-02  2.91197821e-02  6.03811666e-02  3.73494608e-04\n",
      "  1.06575759e-02 -5.96284494e-02 -7.28600919e-02  2.95080561e-02\n",
      "  9.54464264e-03 -2.71541625e-02 -5.63305318e-02  9.66700201e-04\n",
      " -4.77728844e-02  4.67577167e-02  4.87260148e-03 -6.57520220e-02\n",
      " -1.42248441e-02  3.99872698e-02 -1.09798191e-02  7.68942535e-02\n",
      " -4.00003977e-02  2.96826363e-02  2.81303376e-02 -5.55424467e-02\n",
      "  6.31274329e-03  5.00450768e-02  1.89884007e-02  5.38683981e-02\n",
      " -1.95981339e-02  1.08600957e-02  1.64150801e-02  1.44135319e-02\n",
      "  1.71448831e-02  2.17624661e-02 -4.98863682e-02  1.56105757e-02\n",
      "  4.83734906e-03  1.87053811e-02 -3.18548153e-03  2.66863238e-02\n",
      "  5.55552654e-02 -4.88005653e-02 -3.02928910e-02  2.52110381e-02\n",
      "  1.07264733e-02  1.88270397e-02 -1.50688151e-02  3.43831815e-02\n",
      "  4.15124893e-02  1.37788681e-02 -5.54848127e-02  1.43847810e-02\n",
      " -5.88139668e-02 -6.01676032e-02  2.69856453e-02 -5.46130203e-02\n",
      "  8.14632419e-03 -1.17758522e-02  1.57442074e-02  1.43903203e-03\n",
      " -2.64554396e-02 -4.48877327e-02  4.39732708e-02 -1.06181149e-04\n",
      " -2.25905627e-02  3.00296266e-02  1.97440516e-02  7.44073372e-03\n",
      " -1.93790141e-02  8.09793733e-03  4.34860475e-02 -1.08552587e-04\n",
      " -3.77225727e-02  2.67196018e-02 -4.63157818e-02 -1.53401948e-03\n",
      "  8.05305690e-03 -4.30901647e-02 -2.13848874e-02  1.20185502e-02\n",
      "  8.41403380e-03  2.48270505e-03 -3.09566297e-02 -9.05277878e-02\n",
      " -4.76693995e-02  1.22606270e-02 -1.36466688e-02 -2.63654999e-02\n",
      " -7.65550137e-03  8.72373115e-03  2.65724678e-02  8.40092252e-04\n",
      " -5.55933826e-03 -9.29540303e-03  3.19337510e-02  5.94646372e-02\n",
      "  1.83205698e-02 -7.56547153e-02 -5.59389070e-02 -1.20871505e-02\n",
      " -3.16260830e-02  3.62186879e-02  7.53609976e-03 -6.15653880e-02\n",
      " -2.30458695e-02 -3.51678114e-03  1.23331994e-02 -9.67645273e-03\n",
      "  4.96861450e-02 -8.42256695e-02  1.52396280e-02 -1.82445254e-02\n",
      "  7.70461932e-02  9.28718001e-02  4.03725877e-02  1.11732580e-01\n",
      " -1.03270682e-02 -2.54558809e-02  2.13153772e-02 -1.16185483e-03\n",
      "  2.82597076e-03  5.06966896e-02 -3.13697606e-02 -8.14277306e-03\n",
      "  1.38387065e-02  4.66889702e-02  5.09671234e-02  3.77154090e-02\n",
      " -2.94988956e-02  3.60631905e-02 -2.61165621e-03  2.72197329e-04\n",
      " -6.71807081e-02 -6.54026568e-02 -3.43590714e-02  1.91067960e-02\n",
      "  4.13293839e-02 -1.10970698e-02  4.51952554e-02 -5.93564995e-02\n",
      "  1.06963674e-02 -1.82229578e-02 -5.65814152e-02  1.20387189e-02\n",
      "  4.44774777e-02  1.87049657e-02  1.63809936e-02  5.51151074e-02\n",
      " -2.23332345e-02  2.12861691e-02 -1.20339552e-02  3.26752886e-02\n",
      "  1.47003699e-02 -8.16679094e-03  1.12904394e-02 -3.00620571e-02\n",
      " -2.34345403e-02 -2.68646404e-02 -1.28717441e-03 -7.67189860e-02\n",
      "  2.22611520e-03 -5.89484023e-03  2.63103824e-02  2.07131775e-03\n",
      " -6.91151991e-02 -1.43792545e-02  2.68788207e-02 -3.51540297e-02\n",
      " -2.69611962e-02  2.54715607e-03 -6.48881197e-02  3.18727940e-02\n",
      "  1.70126706e-02 -4.54004742e-02 -1.80615708e-02 -1.61116496e-02\n",
      "  5.70773222e-02 -2.78267637e-03 -6.45585954e-02  7.86597952e-02\n",
      "  2.29075532e-02  6.81844773e-03 -9.11736023e-03 -2.27726456e-02\n",
      " -4.76526394e-02  4.88431081e-02 -2.09891386e-02 -2.43694261e-02\n",
      " -5.01209730e-03  6.70254081e-02  6.91369316e-03  2.25842912e-02\n",
      "  2.51125284e-02 -6.92507159e-03  8.59403983e-03  2.38977354e-02\n",
      "  3.29738185e-02 -1.05310582e-01  1.22094573e-02 -1.22263925e-02\n",
      " -5.73768616e-02  1.84311420e-02  2.97157709e-02 -6.09429069e-02\n",
      " -6.55256137e-02  3.55713069e-02  5.64321037e-03  3.34645784e-03\n",
      " -3.59686241e-02 -8.83417018e-03 -6.97895065e-02  6.89779297e-02\n",
      " -4.88214567e-03  2.23995168e-02 -3.16054150e-02 -7.41182640e-03\n",
      "  3.19351517e-02 -5.18788658e-02  2.11601965e-02 -5.03340028e-02\n",
      "  9.10578854e-03  2.13354249e-02  1.66838653e-02  3.49020101e-02\n",
      " -6.38500303e-02 -6.75627403e-03 -1.27405319e-02 -4.63366956e-02\n",
      " -1.14779687e-02  2.08778735e-02  2.44822241e-02  3.66464583e-03\n",
      " -2.86099059e-03  2.29389481e-02  2.13745870e-02 -3.48900892e-02\n",
      " -3.00388094e-02  4.78870682e-02  5.83370514e-02 -9.70497262e-03\n",
      "  1.38234068e-02 -3.27485651e-02 -8.11463862e-04  9.54227988e-03\n",
      "  1.20401718e-02  1.97230782e-02 -4.74880595e-04 -1.39226057e-02\n",
      " -5.21070138e-02 -1.75592452e-02 -5.41698858e-02 -1.17970174e-02\n",
      " -1.71030499e-02 -3.50195095e-02  3.38661224e-02 -6.76587075e-02\n",
      " -2.27606930e-02  1.95606798e-02  5.50249591e-02  1.22028850e-02\n",
      " -1.75164023e-03  7.22441915e-03  1.16349570e-02 -1.61908586e-02\n",
      " -3.37754786e-02  3.22626531e-02 -2.03813687e-02 -2.33859289e-02\n",
      " -1.29991975e-02 -1.66799445e-02  1.03070606e-02 -1.46030281e-02\n",
      " -7.79070258e-02 -8.25812593e-02 -3.38809639e-02  3.81114595e-02\n",
      "  7.86007475e-03  2.41454970e-02 -2.75715292e-02  1.30867576e-02\n",
      " -7.88600184e-03  1.78651568e-02  5.37323393e-02 -3.01822964e-02\n",
      "  1.69455782e-02  1.19571248e-02  3.52554634e-04  4.90208790e-02\n",
      " -8.57211743e-03  1.71267474e-03  4.83878702e-03 -4.10080627e-02\n",
      " -4.68120016e-02 -2.32560490e-03 -5.16776592e-02  3.10030840e-02\n",
      "  1.60961356e-02 -1.00803571e-02 -3.72488261e-03 -3.53388153e-02\n",
      "  2.95961313e-02  2.89097317e-02 -7.59911016e-02 -5.02980761e-02\n",
      " -2.11783517e-02  3.20462547e-02 -3.84537987e-02  2.45102495e-02\n",
      " -2.04188544e-02  6.02114620e-03 -9.81937628e-03  3.74778286e-02\n",
      "  3.40838134e-02  1.28864180e-02  5.67342341e-02 -8.09703469e-02\n",
      " -8.93603824e-03  1.33352866e-02 -2.51565650e-02  2.58413632e-03\n",
      " -6.51803091e-02  1.34400474e-02 -2.04681922e-02  6.53377501e-03\n",
      "  4.56973584e-03  1.99271385e-02 -6.07340075e-02  1.40691884e-02\n",
      " -5.75334132e-02  9.79800243e-03  3.55392955e-02 -2.45283451e-02\n",
      " -4.73310566e-03 -2.77492888e-02  2.34282855e-02 -8.76513732e-05\n",
      "  7.30440766e-03  1.42028704e-02  4.92807329e-02 -3.16540301e-02\n",
      " -1.34902243e-02  3.08487397e-02  2.80402005e-02 -4.33068834e-02\n",
      " -4.42284681e-02  3.80739011e-02  9.47115768e-05 -4.34895828e-02\n",
      "  1.43868160e-02  2.44334131e-03 -4.84072864e-02  1.08955177e-02\n",
      " -9.87493247e-03  4.59295660e-02  3.96379232e-02 -2.60116924e-02\n",
      "  2.48133894e-02 -5.37148975e-02  5.62824868e-02  8.81362241e-03\n",
      "  5.25077358e-02 -1.47371115e-02 -1.74380988e-02  3.45084444e-02\n",
      "  3.75523195e-02 -4.70166989e-02 -1.94911212e-02  3.82631719e-02\n",
      " -5.67596033e-02 -1.78611861e-03  2.33404152e-02 -5.88216455e-33\n",
      " -4.87187952e-02 -2.76265610e-02 -3.38240527e-02  2.66188122e-02\n",
      " -3.39277349e-02 -8.49193521e-03 -1.91250294e-02  3.00252363e-02\n",
      "  3.40781994e-02  5.11157736e-02 -1.92479920e-02  2.85642501e-02\n",
      "  3.66040394e-02  1.68858487e-02  4.77258973e-02  1.23802340e-02\n",
      "  2.14844197e-02  4.93668893e-04  1.21273603e-02 -5.82144037e-02\n",
      "  1.62954479e-02 -7.14260386e-03  4.80092093e-02  2.51190737e-02\n",
      "  4.60097939e-02 -2.29839850e-02 -2.05696486e-02 -3.22233560e-03\n",
      "  4.00091633e-02  3.52309942e-02 -3.43153849e-02  2.75635440e-03\n",
      " -1.25138275e-02  1.97685808e-02  5.53491991e-03  1.03744537e-01\n",
      "  5.77610312e-03 -5.65426424e-02  4.19558920e-02 -3.78831029e-02\n",
      " -3.93443219e-02 -6.24309666e-02 -2.24394095e-03 -5.46548329e-02\n",
      "  4.56134304e-02 -5.69243403e-03  3.38917002e-02 -1.44448206e-02\n",
      "  2.72108847e-03  1.11191245e-02 -5.00661023e-02 -1.61127150e-02\n",
      "  1.72815926e-03  6.88877553e-02  1.16492566e-02  2.83171386e-02\n",
      "  6.97189802e-03  2.68372111e-02 -7.72086810e-03  2.16828734e-02\n",
      "  1.15182791e-02  8.72833058e-02 -6.27273135e-03 -6.44473359e-02\n",
      " -1.58233345e-02  4.03268486e-02 -1.69728305e-02 -1.61188710e-02\n",
      " -3.75576876e-02  7.02938586e-02 -3.30486223e-02  4.66323979e-02\n",
      "  1.18027860e-02  6.51075765e-02 -1.16979117e-02 -8.28348845e-03\n",
      " -5.46905324e-02 -2.00227015e-02  8.42688489e-04 -8.19521956e-03\n",
      "  2.08357349e-02  1.37454169e-02 -1.29924563e-03 -3.94575223e-02\n",
      " -2.00185031e-02 -1.53721767e-02  1.17271943e-02 -4.40110825e-02\n",
      "  5.39267398e-02 -2.33010296e-02 -2.24211477e-02 -3.65212443e-03\n",
      "  2.92212907e-02  7.56443338e-03 -2.90923491e-02  4.01517600e-02\n",
      " -2.00853702e-02 -1.79863803e-03 -1.26235858e-02  2.51076911e-02\n",
      " -4.69285809e-02 -3.08553912e-02 -3.63345403e-04  6.01789821e-03\n",
      "  3.97508852e-02  1.38547504e-02  2.49774475e-02  1.76975839e-02\n",
      " -9.31572989e-02 -9.83684231e-03  8.44928529e-03 -1.95390712e-02\n",
      " -3.26569118e-02  5.13736391e-03  5.80931362e-03  2.08536796e-02\n",
      " -5.97834075e-03  5.86811565e-02 -1.49496561e-02 -5.72965853e-02\n",
      " -5.98236220e-03  1.95203396e-03  2.72975885e-03  6.07001595e-03\n",
      " -2.00525783e-02 -1.31687541e-02 -4.06228490e-02  5.68997227e-02\n",
      "  4.44969311e-02 -1.24308337e-02  1.96967423e-02  3.80979553e-02\n",
      "  2.30237688e-07  1.10575240e-02  4.79513034e-02  6.18299171e-02\n",
      "  4.40278947e-02  6.17665239e-03  2.58289347e-03  3.38913687e-02\n",
      " -5.32937190e-03 -2.59284209e-02 -1.26144392e-02  2.46495605e-02\n",
      " -1.68768747e-03  1.17907743e-03  2.40443088e-02 -9.77309197e-02\n",
      "  1.97368097e-02 -5.52921668e-02 -6.17424771e-02 -4.87151891e-02\n",
      "  1.11081451e-03  1.18732028e-01  8.13257918e-02  3.32448781e-02\n",
      "  4.38326597e-02 -2.49559339e-02 -3.59626561e-02  1.66319311e-02\n",
      "  5.93768293e-03 -1.43971853e-02  4.46710875e-03 -6.01986572e-02\n",
      " -5.65911941e-02 -8.21542647e-03  5.83056593e-03 -1.69482064e-02\n",
      "  9.58631653e-03  1.46733625e-02  5.05845323e-02  3.06891631e-02\n",
      "  6.60468191e-02 -2.56551821e-02 -2.78858393e-02 -3.19173634e-02\n",
      " -3.39236446e-02  1.49903223e-02 -3.03336121e-02 -6.06490346e-03\n",
      " -4.81768791e-03  1.72137227e-02 -8.23369715e-03  1.55548248e-02\n",
      "  2.69106701e-02  5.44308359e-03 -1.06899282e-02 -7.82139879e-03\n",
      " -4.44506593e-02  2.55874228e-02 -5.74760772e-02 -2.05442272e-02\n",
      " -3.07850000e-02 -1.57855060e-02 -7.07540894e-03 -4.21312898e-02\n",
      "  3.79934981e-02  6.27764612e-02 -7.67789269e-03 -3.18353288e-02\n",
      "  1.99277746e-34  1.04834419e-02 -3.39326411e-02  3.93821001e-02\n",
      "  5.53065315e-02  9.42169782e-03  1.09727867e-02 -4.91939969e-02\n",
      "  2.95024086e-02 -8.85374937e-03 -5.96248098e-02 -2.37825569e-02]\n",
      "\n",
      "Sentence: Embeddings are one of the most powerful concepts in machine learning!\n",
      "Embedding: [-2.98611298e-02 -1.37522295e-02 -4.75401804e-02  2.72126794e-02\n",
      "  3.40054780e-02  3.16465683e-02  4.26964164e-02  3.29792430e-03\n",
      "  4.35717441e-02  2.53837071e-02  3.02528907e-02  3.21130902e-02\n",
      " -3.99913304e-02  1.28760887e-02  6.70220032e-02 -7.92899728e-02\n",
      "  4.68771905e-02  2.40266379e-02 -2.07997281e-02 -1.07433265e-02\n",
      " -1.19410511e-02 -5.39290681e-02  4.21055928e-02  2.23588664e-02\n",
      " -2.98949499e-02  8.35983641e-03  1.58384889e-02 -4.80235517e-02\n",
      "  1.88432785e-03 -1.67521685e-02 -2.15629227e-02 -3.88487913e-02\n",
      "  3.06274686e-02  4.20526005e-02  1.69483371e-06 -1.86928883e-02\n",
      " -1.24558657e-02  1.32128838e-02 -4.89040166e-02  1.34746563e-02\n",
      "  2.28873137e-02  8.81779008e-03  8.64928402e-03 -2.00949665e-02\n",
      " -3.15217786e-02 -2.53433194e-02  7.57319033e-02  3.62446420e-02\n",
      "  1.25290249e-02  3.09694838e-02  4.50760080e-03 -3.50041948e-02\n",
      " -4.42513759e-04 -9.76648554e-03  6.04545660e-02  4.03472148e-02\n",
      "  1.10734589e-02  6.56206021e-03 -5.84597979e-03  3.79780331e-03\n",
      " -4.46915068e-02  1.76405255e-02  2.45917160e-02 -3.60039040e-03\n",
      "  1.02473378e-01  3.73759195e-02  6.13312051e-03 -2.24676225e-02\n",
      "  1.46482317e-02  5.00536934e-02 -2.29907427e-02  1.12924855e-02\n",
      " -3.10552567e-02 -1.49509469e-02 -2.53131916e-03  3.20944674e-02\n",
      " -4.67056334e-02 -4.85887155e-02  2.98305955e-02  6.44216016e-02\n",
      " -3.12613547e-02  3.57407220e-02  4.16527241e-02 -5.52517287e-02\n",
      " -8.74628965e-03 -2.18630414e-02 -1.12744654e-02 -2.14436464e-02\n",
      " -1.32824071e-02 -2.04865802e-02 -1.00575760e-02  3.54763754e-02\n",
      " -7.47604109e-03 -3.70188579e-02  5.77893406e-02 -2.18168776e-02\n",
      "  4.36225999e-03  2.04380509e-02  3.36814895e-02 -4.92800735e-02\n",
      "  4.82793562e-02 -1.80999585e-03 -1.05119124e-02  4.13322933e-02\n",
      " -6.79833218e-02  1.75716020e-02 -4.43412624e-02  9.90839396e-03\n",
      " -3.81810218e-02  1.10827265e-02 -5.07279262e-02 -2.17451174e-02\n",
      " -1.03836162e-02  4.60331887e-02  1.55862644e-02 -4.21366617e-02\n",
      " -2.72146408e-02  3.22818421e-02 -4.24739420e-02  2.71207001e-02\n",
      " -7.41061345e-02  4.20107245e-02  2.02437714e-02  7.31810927e-02\n",
      " -8.97692703e-03 -2.31160074e-02 -3.93559709e-02 -1.46008665e-02\n",
      " -3.30910273e-02  1.12239886e-02  2.58563343e-03 -4.36853012e-03\n",
      "  1.85855366e-02  2.69934740e-02 -1.67215057e-02  3.69570106e-02\n",
      "  4.44489159e-02 -2.21724045e-02  6.72969734e-03  1.22935548e-02\n",
      "  1.71758141e-02 -2.36472371e-03  3.72263975e-02 -2.22870763e-02\n",
      "  2.94603556e-02 -2.33691148e-02  5.38469339e-03 -3.06581352e-02\n",
      " -2.38920115e-02 -2.63614468e-02 -2.01789159e-02  1.11245655e-01\n",
      " -1.99837107e-02 -3.54029909e-02  3.84143405e-02  2.53069252e-02\n",
      "  1.99551135e-02  5.53518273e-02 -1.99332479e-02 -2.16719578e-03\n",
      "  4.91092913e-02 -4.03530896e-02 -1.16977217e-02 -5.33113480e-02\n",
      "  8.29585735e-03 -5.08251637e-02 -2.65503749e-02 -1.53242443e-02\n",
      "  5.78811485e-03  2.46574241e-03 -3.44449207e-02 -1.85131433e-03\n",
      " -3.95730324e-02 -2.71690655e-02  4.93568033e-02  8.38369057e-02\n",
      "  5.43491840e-02  8.22260901e-02  1.23895118e-02 -4.79795644e-03\n",
      "  7.77329609e-04  2.98486128e-02 -1.85584854e-02  5.98795190e-02\n",
      " -6.82790158e-03  9.78158903e-04  2.85485983e-02 -7.64615741e-03\n",
      " -1.86620206e-02 -2.69287527e-02 -2.90333051e-02 -1.37871206e-02\n",
      " -2.57598888e-03 -2.20173504e-02 -1.70821808e-02 -3.81843336e-02\n",
      "  2.21505705e-02 -3.59234437e-02 -1.19439177e-02 -3.18307951e-02\n",
      " -4.80801202e-02  9.77635663e-03 -1.04863488e-03  4.15372178e-02\n",
      " -1.10974098e-02 -4.72829714e-02  1.90984681e-02 -5.31177409e-02\n",
      "  2.11326610e-02 -2.53062299e-03  5.61055429e-02 -1.33795235e-02\n",
      " -5.95859019e-03 -1.20307952e-02  4.63929735e-02 -2.81909127e-02\n",
      "  2.25355253e-02 -2.50471756e-03 -3.52453180e-02  2.55494677e-02\n",
      "  9.10398364e-03  3.25213093e-03  2.55970308e-03 -1.25623867e-02\n",
      " -3.51496451e-02 -4.28946316e-02 -2.32333038e-03  2.41021067e-02\n",
      " -5.16840490e-03  1.68739986e-02  5.52647561e-03  2.36792061e-02\n",
      "  5.65164462e-02 -3.47868837e-02 -6.34517372e-02 -7.45628355e-03\n",
      " -1.78447999e-02  5.35898060e-02  2.67291702e-02 -8.74199420e-02\n",
      "  1.04195690e-02 -4.14009846e-04 -3.04448442e-03  9.14251711e-03\n",
      "  2.91529298e-02 -5.81832118e-02  6.83463588e-02 -4.08618227e-02\n",
      " -9.09791328e-03 -3.40769701e-02  3.52410823e-02 -1.02628088e-02\n",
      " -5.72499936e-04 -2.73453956e-03  1.59635972e-02  4.49071079e-03\n",
      " -2.09051687e-02  3.02769467e-02  2.46119462e-02 -1.44067043e-02\n",
      "  1.73268430e-02  1.99031923e-03  4.23051305e-02 -2.39177290e-02\n",
      " -3.25547419e-02 -1.45939635e-02  3.95101160e-02 -6.04649931e-02\n",
      " -3.02065872e-02  1.67189240e-02 -2.26817522e-02 -2.61955243e-02\n",
      " -5.51320277e-02  1.44908372e-02 -1.99246816e-02  3.99751170e-03\n",
      "  3.12609486e-02 -4.90727797e-02 -9.49708745e-04  5.39496951e-02\n",
      " -9.10082180e-03 -2.69486848e-02 -3.63159366e-02 -1.38437087e-02\n",
      " -4.45622057e-02  5.49358837e-02  2.17592740e-03  2.23443937e-03\n",
      " -5.23022283e-03 -1.47894537e-02  3.60592119e-02  1.45262741e-02\n",
      "  8.39189719e-03 -6.10361025e-02 -7.89949019e-03 -2.98296171e-03\n",
      "  3.56563344e-03  8.33992288e-02 -2.61215251e-02  8.06722417e-02\n",
      "  3.63061577e-03  1.69974249e-02  2.58604400e-02  1.09442289e-03\n",
      " -4.57063168e-02  5.55678718e-02  2.00643167e-02  4.76660766e-02\n",
      " -4.91054021e-02 -1.86080672e-02  3.34404409e-02 -2.57310197e-02\n",
      " -3.16368882e-03  7.21444264e-02 -1.61519237e-02 -1.33934170e-02\n",
      " -6.06294051e-02 -2.82187331e-02 -8.91926512e-03 -2.71168258e-03\n",
      "  8.04918725e-03 -4.95210104e-02  7.89434612e-02  2.76428089e-02\n",
      " -5.42582199e-03 -3.06722755e-03 -4.11826745e-02  1.39172664e-02\n",
      "  3.04253083e-02  1.02856224e-02  1.06679238e-02 -5.56554012e-02\n",
      " -1.75082888e-02  2.03868467e-02  8.43309518e-03  3.82471234e-02\n",
      " -3.89099978e-02 -1.61302797e-02  3.18059437e-02 -7.32970238e-02\n",
      " -1.76502038e-02 -4.79874648e-02 -5.55042289e-02 -5.00750821e-03\n",
      "  4.46735125e-04  3.57333235e-02 -8.24693649e-04 -3.34324166e-02\n",
      " -3.32417116e-02 -2.46460158e-02  2.15332117e-02  3.90854664e-03\n",
      "  2.53471173e-02  6.02989690e-03 -7.81609956e-03  1.23765692e-02\n",
      " -1.71039384e-02  2.68103313e-02  2.83678970e-03 -1.27643887e-02\n",
      "  1.00510858e-01  1.03580896e-02 -3.55143212e-02  1.56616494e-02\n",
      " -9.85949636e-02  4.58441079e-02 -3.15230601e-02 -2.35781092e-02\n",
      " -2.78350152e-02 -7.75599765e-05 -2.82363184e-02 -1.92918163e-02\n",
      "  1.87389236e-02  5.71941100e-02  2.56912112e-02 -3.20030488e-02\n",
      "  1.99074559e-02 -3.15819792e-02 -4.02061455e-02  5.77630401e-02\n",
      "  1.72974635e-02 -5.37013374e-02 -1.25325937e-02 -1.45483892e-02\n",
      " -5.76174408e-02  1.09727466e-02 -2.04727650e-02  2.85540558e-02\n",
      " -5.04399762e-02  4.36991155e-02  1.75711010e-02 -1.02343475e-02\n",
      " -9.69771743e-02 -2.99995486e-02 -2.86679715e-02  2.24936921e-02\n",
      " -1.68121271e-02 -1.43673671e-02 -8.79589096e-03 -1.69043969e-02\n",
      "  2.41557714e-02 -6.53192326e-02 -4.10800241e-02 -2.34057792e-02\n",
      " -6.76064938e-02 -1.55690098e-02  3.62358093e-02  7.83159807e-02\n",
      " -4.97516580e-02 -7.08547533e-02 -5.01179807e-02 -8.56448547e-04\n",
      "  5.44898445e-03  4.34705894e-03  9.88053009e-02 -2.16416027e-02\n",
      " -1.87750645e-02  1.15069374e-02  2.63996385e-02  1.65235754e-02\n",
      " -2.24057902e-02 -4.31826673e-02  1.31803885e-01 -2.97034029e-02\n",
      "  2.65934039e-02 -1.38888927e-02 -1.67003535e-02  3.44145633e-02\n",
      " -8.94354749e-03  6.16001524e-02 -3.42302956e-02  2.46427534e-03\n",
      " -8.14094488e-03  5.80325127e-02  5.24208732e-02 -1.53281158e-02\n",
      "  4.01382707e-02  1.51407002e-02 -3.01464787e-03 -4.97020893e-02\n",
      " -4.24303347e-03  5.77288978e-02  3.17873918e-02  4.74008098e-02\n",
      "  2.95218136e-02 -1.50123192e-02 -2.47945301e-02 -7.11502135e-02\n",
      "  2.06848253e-02  3.11487820e-02 -5.86067513e-03  1.62786841e-02\n",
      " -3.93683314e-02  5.46505786e-02  3.26595269e-02 -1.87021624e-02\n",
      " -9.79862884e-02  4.33774199e-03 -5.58157042e-02 -1.34620722e-02\n",
      "  2.88454164e-02  1.58748329e-02 -3.32564488e-02  1.44419423e-03\n",
      " -5.51108271e-02  8.24674517e-02  2.38846187e-02 -2.04838216e-02\n",
      " -4.78587532e-03  3.78722437e-02 -4.87563200e-02  3.44647355e-02\n",
      "  1.10358335e-02  1.12450095e-02  1.33262901e-02 -3.46375294e-02\n",
      " -6.92220703e-02  7.30118295e-03 -6.57012081e-03  1.73203889e-02\n",
      "  5.23055950e-03  4.48132195e-02  3.89852487e-02 -1.99274682e-02\n",
      " -1.80920325e-02  3.25938165e-02 -2.02027429e-02  4.86268022e-04\n",
      " -8.88758060e-03 -1.91348083e-02  2.50685923e-02  4.74019721e-02\n",
      "  2.18654773e-03 -1.69988051e-02  3.62670645e-02  3.46248224e-03\n",
      "  4.21926891e-03  8.04171115e-02  3.10627483e-02 -1.04948005e-03\n",
      " -3.55466790e-02  4.34836745e-02 -3.06218583e-02 -3.03191859e-02\n",
      " -4.13175300e-02 -1.05258096e-02 -2.35242117e-02 -1.86772328e-02\n",
      "  4.42936411e-03  5.45056351e-02 -6.05017617e-02  2.48422064e-02\n",
      " -3.36967669e-02 -4.54169102e-02 -2.63173208e-02  6.98049460e-03\n",
      "  6.92871362e-02 -2.04491671e-02 -1.96812525e-02 -9.72562470e-03\n",
      " -1.21564772e-02  7.89342169e-03  1.84751698e-03 -6.93651289e-02\n",
      "  2.43357010e-02  4.00609858e-02  3.44013460e-02 -2.84281820e-02\n",
      " -1.09431939e-02  1.38743082e-02 -4.40584356e-03  1.19347346e-03\n",
      " -8.81165192e-02  1.15930997e-02 -2.56350804e-02  5.57525307e-02\n",
      "  1.26946211e-01  5.39565869e-02 -1.41437352e-02  1.27196237e-02\n",
      " -1.32235875e-02 -5.94484434e-02  2.86703892e-02  2.57284530e-02\n",
      " -8.33777711e-03  8.17378168e-04  5.93051594e-03  3.29110511e-02\n",
      "  4.12751846e-02 -5.77967474e-03 -1.71124041e-02  1.06227724e-02\n",
      " -2.19601803e-02 -4.97207157e-02  2.53765974e-02 -5.60257542e-33\n",
      " -1.35397390e-02 -3.77958566e-02 -2.67920736e-03 -3.69713118e-04\n",
      " -1.98267400e-02  5.47053013e-03  6.02687476e-03  1.93069000e-02\n",
      "  3.87979788e-03  2.97698714e-02 -1.88228823e-02  2.20034691e-03\n",
      "  8.17021821e-03  1.61965042e-02  3.17529440e-02 -6.83416566e-03\n",
      "  2.19252165e-02  4.37759620e-04  2.96859145e-02 -2.62557194e-02\n",
      "  6.49378495e-03  3.56025361e-02  1.58610975e-03 -4.76583950e-02\n",
      " -5.26238605e-02  3.78274582e-02  3.54341827e-02 -3.10347620e-02\n",
      "  7.96406623e-03  5.48469797e-02 -3.56443636e-02  9.10137035e-03\n",
      " -9.45984572e-03 -4.63286862e-02 -1.63906589e-02  6.32452592e-02\n",
      " -1.38587989e-02 -5.95724471e-02 -1.57990921e-02  2.01886743e-02\n",
      " -1.98292732e-02 -3.49211767e-02  2.27937903e-02 -5.91621920e-02\n",
      "  4.18854952e-02  1.20738673e-03  5.19158989e-02 -1.88435633e-02\n",
      " -3.12102027e-02  2.34932732e-02 -7.41030127e-02 -2.76572217e-04\n",
      " -1.51719600e-02  6.11713603e-02  1.25065103e-01 -1.28459120e-02\n",
      " -1.12670995e-02  1.51751481e-03 -8.09153542e-02  1.12689221e-02\n",
      " -1.97573397e-02  2.74268277e-02  9.40998271e-03 -9.58756451e-03\n",
      "  2.54849847e-02  6.81658909e-02 -1.83452945e-02 -1.00963928e-01\n",
      " -9.45244916e-03 -5.27013186e-03  1.98684372e-02  9.80847180e-02\n",
      "  3.15633304e-02  5.30423038e-02  3.75123471e-02 -6.64208233e-02\n",
      " -5.92879727e-02 -1.57074071e-02  1.76609159e-02 -5.81072941e-02\n",
      "  2.23230757e-02  1.29869478e-02 -3.30261216e-02  9.96891176e-04\n",
      " -9.87090450e-03 -3.12955566e-02  2.28527794e-03 -4.91250753e-02\n",
      "  1.47693958e-02 -1.83367003e-02 -4.16306220e-02  3.76896895e-02\n",
      "  3.35410349e-02 -7.97111392e-02  4.01299335e-02  1.59071945e-02\n",
      "  5.06082177e-03  4.28808220e-02  2.29760129e-02 -4.13351133e-02\n",
      " -3.10503896e-02 -5.26405126e-02 -4.95404713e-02 -2.94256788e-02\n",
      "  5.94924353e-02 -2.59802677e-02  3.02497502e-02  8.80404934e-03\n",
      " -4.84466888e-02 -2.00851467e-02  9.82179679e-03 -7.89194182e-02\n",
      "  4.52878326e-03 -9.34894755e-03  9.23314970e-03 -3.17361280e-02\n",
      "  2.10833102e-02  6.37119962e-03  3.36347893e-02  3.83613668e-02\n",
      " -4.55527715e-02  1.08121953e-03 -9.83324368e-03  7.70195341e-03\n",
      " -2.87617426e-02 -1.74959376e-02 -4.27812058e-03  2.81287078e-02\n",
      "  4.97339927e-02 -7.45570734e-02 -1.07008899e-02 -7.66055286e-03\n",
      "  2.33968819e-07  1.52483983e-02  8.39613751e-02  3.67242843e-02\n",
      " -3.69248688e-02  3.64751369e-02  4.26422134e-02 -4.39830916e-03\n",
      "  1.78133678e-02 -2.67076548e-02 -7.13903829e-03  5.59975505e-02\n",
      "  3.13966572e-02  2.13442580e-03  3.90371382e-02 -8.78527984e-02\n",
      " -2.21660156e-02 -2.47735362e-02 -1.18189473e-02 -7.89706130e-03\n",
      " -2.08857600e-02  4.30556126e-02  1.07643589e-01  4.40618433e-02\n",
      "  1.47962570e-02  2.44862996e-02 -3.86268161e-02  1.80743709e-02\n",
      " -1.47839054e-03  7.74166957e-02 -4.19565700e-02 -3.80529016e-02\n",
      "  3.61257158e-02  1.59033097e-03  1.95324216e-02 -2.00080592e-02\n",
      "  4.22538109e-02  3.06110717e-02 -3.53689282e-03  5.93105424e-03\n",
      " -2.23223679e-02 -2.07131449e-02 -3.62908957e-03  1.74653567e-02\n",
      " -4.08759080e-02  5.91594726e-02 -5.89100122e-02 -3.96753550e-02\n",
      " -3.33528779e-02  1.02161895e-02  6.97292760e-03  7.70389661e-02\n",
      " -1.86911896e-02 -1.82568487e-02 -2.42318790e-02 -3.40699218e-03\n",
      " -3.60555425e-02  4.33389321e-02 -3.48602496e-02  5.27768396e-02\n",
      "  2.89709345e-02 -4.98462543e-02 -1.94748808e-02  1.16397394e-02\n",
      " -3.04614771e-02  8.04637671e-02  6.56247959e-02 -2.84532998e-02\n",
      "  1.81615255e-34 -4.19158861e-03 -2.57882755e-02  5.17320521e-02\n",
      "  4.94420901e-02  1.32476278e-02 -4.21994254e-02 -1.12458542e-02\n",
      " -2.61519346e-02  5.51130138e-02  2.20024791e-02 -2.51170117e-02]\n",
      "\n",
      "Sentence: Learn to use embeddings well and you'll be well on your way to being an AI engineer.\n",
      "Embedding: [-2.20730696e-02  2.08950602e-02 -6.03005402e-02  8.43944959e-03\n",
      "  4.37650904e-02  1.55070489e-02  4.99907769e-02 -3.03232390e-02\n",
      "  4.94784415e-02  2.35512070e-02  3.29350978e-02  1.53877698e-02\n",
      " -6.68355003e-02  1.11002862e-01  6.92677274e-02 -2.31888462e-02\n",
      "  3.79102863e-02 -4.94145369e-03 -1.57800093e-02 -3.45476381e-02\n",
      " -2.65052821e-02 -2.47879606e-02 -1.86141580e-02  3.00361961e-02\n",
      " -2.81186271e-02 -8.75132810e-03 -3.30772367e-03 -2.06115842e-02\n",
      "  1.03315609e-02 -1.51483035e-02 -3.48331146e-02 -2.63247881e-02\n",
      "  2.06907708e-02  3.79108898e-02  1.81912912e-06 -2.44283979e-03\n",
      " -1.80559547e-03  5.61763439e-03 -2.79870220e-02  1.54703045e-02\n",
      "  3.06456685e-02  3.72600965e-02 -1.55611485e-02  2.54414361e-02\n",
      " -6.42072409e-02  3.16353031e-02  6.63442761e-02  3.80970016e-02\n",
      "  5.57844527e-02  5.31659909e-02 -9.69290547e-03 -3.61423977e-02\n",
      "  3.72434780e-02 -4.67827916e-03  5.14575318e-02  1.00058336e-02\n",
      "  4.90289275e-03  1.41562587e-02  4.95099984e-02  3.32950242e-03\n",
      " -3.21100466e-02  4.42388169e-02  3.27415951e-02 -7.90622737e-03\n",
      "  1.07809775e-01  7.32945278e-02  3.36702578e-02 -4.28345762e-02\n",
      "  1.05966292e-02  2.05654148e-02 -2.02670060e-02  1.04964403e-02\n",
      " -1.97615810e-02 -2.89427735e-05 -2.61862762e-02 -1.85173992e-02\n",
      " -3.44269648e-02 -4.08621393e-02  2.32571475e-02  2.14195978e-02\n",
      "  1.31320991e-02 -3.27211134e-02 -1.91425551e-02 -2.86572315e-02\n",
      " -1.16859311e-02  1.21910339e-02  1.05248252e-02 -3.39585021e-02\n",
      "  3.08904983e-03 -4.44888175e-02  2.65104771e-02  1.09536033e-02\n",
      "  2.51450855e-02 -6.48838049e-03  4.54374915e-03 -2.02785041e-02\n",
      " -1.03216209e-02  2.06589811e-02 -1.65313520e-02 -2.45612431e-02\n",
      "  5.47549538e-02  2.68261526e-02  2.95510516e-02  3.86754610e-02\n",
      " -7.76720271e-02  3.80055420e-02 -2.98364740e-02  7.96886086e-02\n",
      " -3.00943423e-02  7.57847214e-03 -6.89826459e-02 -2.92667020e-02\n",
      " -2.35579815e-02  3.48198488e-02  2.52938475e-02 -4.53817435e-02\n",
      " -1.57938916e-02  4.39031310e-02 -4.04335931e-02  8.32530670e-03\n",
      " -2.84665357e-02  4.94933799e-02  2.41276585e-02  3.02192327e-02\n",
      " -4.99590337e-02 -5.94533011e-02 -3.70175876e-02  1.30330706e-02\n",
      " -3.36468592e-02  3.45589630e-02 -1.44523522e-02  2.57639922e-02\n",
      "  4.61183209e-03  2.21551824e-02 -4.93460661e-03  9.66004878e-02\n",
      " -2.72450107e-03  5.65343711e-04 -3.24248038e-02  1.31681813e-02\n",
      "  4.41607460e-02 -7.03050522e-03  6.84261397e-02 -2.28166524e-02\n",
      " -2.81028752e-03 -4.23883088e-02 -1.33632049e-02 -5.96738867e-02\n",
      " -6.96125906e-03 -2.31900904e-02 -3.78851704e-02  9.80186611e-02\n",
      " -2.21728906e-02 -2.30062529e-02  3.22815068e-02  8.21804535e-03\n",
      " -7.04121683e-03  4.84079309e-02  4.23291475e-02 -2.59715319e-03\n",
      "  1.20520235e-04  1.67413894e-02  2.91197877e-02 -1.28740715e-02\n",
      " -2.41077784e-02 -3.29319611e-02 -3.50291724e-03 -3.19322012e-02\n",
      " -2.64171977e-02  2.30404288e-02  1.11637255e-02 -9.95999109e-03\n",
      " -1.75901093e-02 -2.00276705e-03  1.21595236e-02  4.67823781e-02\n",
      "  5.20882159e-02  7.21171945e-02  1.94978695e-02  1.15072411e-02\n",
      "  5.94165362e-03 -1.47833256e-02 -2.87724212e-02  6.72666207e-02\n",
      " -1.68758743e-02  5.25876414e-03 -3.39737348e-02  5.24596162e-02\n",
      " -2.59793233e-02 -4.41379733e-02  1.47449796e-03 -1.06599089e-02\n",
      " -1.51858972e-02 -1.55874877e-03  1.81505140e-02 -4.85411398e-02\n",
      "  3.67909390e-03 -6.59313202e-02 -1.49418330e-02 -3.23529392e-02\n",
      " -2.79949717e-02  1.71856787e-02 -7.87091535e-03  4.65692356e-02\n",
      "  1.47123318e-02 -7.40438774e-02 -6.52104169e-02 -5.22735268e-02\n",
      " -1.82343069e-02  5.20859025e-02  3.06304879e-02 -2.36037150e-02\n",
      "  2.42384858e-02 -1.83939673e-02 -4.83013690e-03 -2.13386696e-02\n",
      "  1.56583562e-02  9.87337995e-03 -4.25561629e-02  6.00460824e-03\n",
      " -3.14501440e-03  4.51511284e-03 -1.52780465e-03  1.13731483e-02\n",
      " -6.96354359e-02 -3.37257385e-02  1.33406715e-02  4.87289997e-03\n",
      " -7.81494007e-03  4.78049517e-02 -1.59712005e-02  3.14606428e-02\n",
      "  5.15920594e-02 -4.05122563e-02 -5.06461076e-02  9.99930408e-03\n",
      " -2.00729482e-02  4.21552584e-02  3.03182285e-02 -1.00431271e-01\n",
      " -4.12019752e-02  3.43990289e-02  3.29208896e-02  1.07411738e-03\n",
      "  3.70962135e-02 -6.94237202e-02  6.52394146e-02  8.31664260e-03\n",
      "  1.68036073e-02 -2.60706004e-02  8.14495888e-03 -1.48009583e-02\n",
      " -2.65671164e-02  3.29321288e-02 -7.37511599e-03  7.23974872e-03\n",
      " -2.69329622e-02  1.71754509e-02 -2.28083134e-02 -4.75343410e-03\n",
      "  2.88568791e-02  1.30792178e-04  5.44128492e-02 -1.43378722e-02\n",
      "  1.89891644e-02 -1.32732177e-02  4.01177369e-02 -7.29275793e-02\n",
      " -2.41210945e-02  3.16216834e-02 -1.68014411e-02  8.47543217e-03\n",
      " -5.23940735e-02 -1.43882707e-02 -1.46156540e-02  6.39907038e-03\n",
      "  2.15113312e-02 -5.18960431e-02 -4.30575907e-02  2.34075896e-02\n",
      "  2.30272138e-03 -2.48433948e-02 -4.38243151e-02 -2.16570888e-02\n",
      " -6.91595376e-02  1.76770259e-02  2.12067757e-02 -2.20293850e-02\n",
      " -1.06773470e-02  9.57427267e-03  2.21988540e-02  5.51470704e-02\n",
      "  1.03682783e-02 -8.14825147e-02 -7.94705469e-03 -1.85866226e-02\n",
      "  1.20494300e-02  7.51403496e-02 -1.41215697e-02  8.83925110e-02\n",
      "  3.12628224e-02  8.12261645e-03 -2.29444467e-02  3.96010466e-02\n",
      " -2.00167876e-02  9.16027576e-02 -2.06839330e-02  5.84990494e-02\n",
      " -4.32369001e-02 -4.74751554e-03 -9.51890927e-03  5.42952027e-03\n",
      "  1.19119133e-04  6.15376942e-02 -1.68786931e-03 -4.66321222e-02\n",
      " -2.01405324e-02  1.32406168e-02  9.70686134e-03  2.73847487e-02\n",
      "  3.06639690e-02  6.71578338e-03  8.71221051e-02 -1.97371165e-03\n",
      "  8.18297267e-03  5.44365449e-03 -5.76205924e-02  1.34821357e-02\n",
      "  5.06284088e-03 -2.10568793e-02  1.25937229e-02 -5.49777504e-03\n",
      " -1.44645199e-02 -2.92567108e-02  5.53299040e-02 -2.60099657e-02\n",
      " -2.82455632e-03 -2.30902173e-02  8.89710244e-03 -2.61565316e-02\n",
      "  9.08614486e-04 -6.16204180e-02 -7.56419003e-02 -1.05932495e-02\n",
      " -1.19563593e-02  6.71458617e-02 -1.96234751e-02 -5.00934012e-02\n",
      " -3.91229130e-02 -3.07008363e-02  7.18906298e-02  9.29243769e-03\n",
      " -6.34387461e-03  7.86934863e-04 -1.36484019e-02  2.87188664e-02\n",
      "  4.01224233e-02  1.28037175e-02  1.77381393e-02 -4.75975825e-03\n",
      "  5.47173880e-02  1.10811321e-03 -2.25790497e-02 -2.80290213e-03\n",
      " -1.13696091e-01  2.55904552e-02  4.00451565e-04 -4.39810604e-02\n",
      "  1.36318700e-02 -1.54137490e-02 -4.99015115e-02 -2.32889652e-02\n",
      " -1.62987714e-03  3.95835340e-02  1.89040359e-02 -3.02703157e-02\n",
      "  2.71438099e-02  1.05683180e-03 -4.21068482e-02  3.71961407e-02\n",
      "  3.54258381e-02 -6.98274896e-02 -2.20937636e-02 -4.01495695e-02\n",
      " -1.90164242e-02 -2.69835424e-02 -1.51212979e-02  3.33365239e-02\n",
      " -9.74889547e-02  1.73102710e-02  6.21014461e-03 -2.59420578e-03\n",
      " -1.10152937e-01 -6.10186681e-02 -1.36549678e-02 -1.35034714e-02\n",
      " -6.72574192e-02 -4.05120384e-03 -6.64987229e-03  3.86570371e-03\n",
      "  9.43472330e-03 -3.86637039e-02 -1.93593465e-02  1.34933759e-02\n",
      " -4.58106659e-02  6.06737398e-02  6.06380180e-02  4.85596135e-02\n",
      " -4.56089005e-02 -5.71936369e-02 -1.55094927e-02  3.40962969e-02\n",
      "  9.48142435e-04 -9.94346850e-03  2.84658000e-02 -3.29024568e-02\n",
      " -2.83211321e-02  3.19907479e-02  2.61298884e-02 -2.74055023e-02\n",
      " -1.36353020e-02  7.47711724e-03  1.19430274e-01 -4.45807129e-02\n",
      "  1.07671963e-02 -8.69424120e-02 -2.19550971e-02  1.83874685e-02\n",
      " -1.06521444e-02 -1.89242642e-02 -3.06514353e-02 -3.04701999e-02\n",
      " -3.22214924e-02  4.12150919e-02  8.95755459e-03 -2.73179654e-02\n",
      "  9.39996820e-03 -9.57171433e-04 -1.94010511e-02 -4.92622554e-02\n",
      " -9.18894541e-03  4.66894098e-02  5.41893095e-02  2.21609380e-02\n",
      " -2.86351796e-02  5.20295389e-02  2.47589406e-02 -7.14267939e-02\n",
      " -1.26206530e-02  7.35525135e-03  2.13785004e-02  2.93513853e-02\n",
      " -2.57651154e-02  5.20562157e-02 -2.74891797e-02 -3.10242679e-02\n",
      " -9.02880505e-02  6.10371344e-02 -5.22610173e-02  2.13111322e-02\n",
      "  4.41735461e-02  3.23370732e-02  1.75648164e-02 -2.39519533e-02\n",
      " -2.69709621e-02  5.11278622e-02  2.69064754e-02 -4.51932512e-02\n",
      "  2.52656452e-03  2.44934447e-02 -2.89541315e-02  2.79992465e-02\n",
      " -1.36022400e-02 -4.32368368e-02  1.85830183e-02  7.63723583e-05\n",
      "  2.43508420e-03 -3.73326242e-03 -1.72279738e-02  1.01292599e-02\n",
      "  1.98437013e-02 -2.60018203e-02 -3.40172439e-03  1.09125702e-02\n",
      " -4.16363440e-02  3.37032080e-02 -2.81634703e-02  1.79126151e-02\n",
      " -4.53095324e-02 -1.09819239e-02 -2.20830808e-03  1.99102368e-02\n",
      "  3.56371962e-02 -3.11798919e-02  3.78752314e-02 -1.41408807e-02\n",
      " -2.16907766e-02  2.73019746e-02  3.69815505e-03  6.35387450e-02\n",
      "  1.22669069e-02 -6.02269499e-03 -7.60757644e-03 -1.86565239e-02\n",
      " -5.64720389e-03 -2.20050057e-03 -1.31825712e-02  1.67724192e-02\n",
      " -3.77264433e-02  2.97895502e-02 -5.01569696e-02  4.89088185e-02\n",
      " -6.07445054e-02 -8.39419216e-02 -5.09001054e-02  1.81767903e-02\n",
      "  6.66732565e-02 -3.30039649e-03 -2.82402546e-03 -5.35406061e-02\n",
      "  3.90340984e-02  2.19852235e-02  3.13555226e-02 -3.36527117e-02\n",
      "  1.96913593e-02  1.67883243e-02  5.04002869e-02  3.08060809e-03\n",
      " -7.24796788e-04  4.42907177e-02 -4.12960164e-03  4.29328792e-02\n",
      " -6.62552565e-02  1.16057752e-03 -2.81716380e-02  1.56885553e-02\n",
      "  9.78134051e-02  5.53594828e-02 -1.39378719e-02  2.12307200e-02\n",
      " -1.30955633e-02 -6.82028010e-02 -8.09093763e-04  4.99291420e-02\n",
      " -2.69265566e-02 -2.97804028e-02  3.84461954e-02  1.97354779e-02\n",
      "  3.37088592e-02  1.65873952e-02  5.77315176e-03 -3.04897726e-02\n",
      " -1.52512053e-02 -3.56159247e-02 -8.69223196e-03 -5.42296980e-33\n",
      "  3.24372738e-03 -3.46329808e-02  3.58932763e-02  1.83770899e-02\n",
      " -2.17505023e-02 -3.26411277e-02  2.88397889e-03  1.50463758e-02\n",
      " -1.75263418e-03 -1.99418515e-02 -6.10358128e-03  2.23847236e-02\n",
      " -8.78933468e-04  2.48684995e-02  3.39736603e-02  2.75593046e-02\n",
      "  3.37792374e-02  3.98564637e-02  2.55545452e-02  1.83042772e-02\n",
      " -2.92878300e-02  5.18082501e-03  8.37778673e-04 -3.66560407e-02\n",
      " -3.46732773e-02  3.82687338e-02  5.50824124e-03 -4.35187593e-02\n",
      "  2.44077798e-02  3.54167596e-02 -2.13442482e-02  2.86623891e-02\n",
      " -2.65395473e-04  3.73409204e-02 -8.68168846e-03  3.04787746e-03\n",
      " -2.71681994e-02 -3.85088101e-02 -6.12388663e-02 -2.00845604e-03\n",
      " -1.22080268e-02 -8.67197737e-02  3.75361042e-03 -1.77707709e-02\n",
      "  8.32478702e-03 -1.69167984e-02  7.02403933e-02  3.32233980e-02\n",
      "  4.34312746e-02  1.47016915e-02 -1.25546902e-01  1.50866620e-02\n",
      " -5.43164499e-02 -1.79150049e-03  4.99600992e-02 -1.53786056e-02\n",
      "  3.32683437e-02 -3.07708625e-02 -1.83897167e-02  9.45798028e-03\n",
      " -4.60290834e-02 -2.03867955e-03  2.62428913e-02 -5.00789545e-02\n",
      "  2.01835819e-02  6.08983040e-02 -2.01180708e-02 -2.60054152e-02\n",
      "  1.05925631e-02 -3.31153870e-02  1.62595715e-02  7.77862221e-02\n",
      " -1.90730777e-03 -5.62890526e-03  1.43716093e-02 -4.06833962e-02\n",
      " -5.14971279e-02  1.66216254e-04 -3.33058881e-03  1.44688850e-02\n",
      "  4.24964179e-04  3.04453131e-02 -1.83636881e-02  1.51187275e-03\n",
      "  2.99861319e-02 -3.68001908e-02  8.35626666e-03 -3.31025459e-02\n",
      "  2.66911592e-02  5.47831412e-03 -1.80524494e-02  2.42577549e-02\n",
      "  5.72710345e-03 -5.93372434e-02  1.04358479e-01 -9.87924356e-03\n",
      " -1.36105847e-02  5.79998754e-02  2.50108540e-02  2.89337113e-02\n",
      " -3.20521854e-02 -3.40233184e-02 -3.41698788e-02 -2.76981313e-02\n",
      "  6.47003278e-02  1.50797674e-02 -1.61925126e-02  3.03266048e-02\n",
      " -2.67188270e-02 -3.67774256e-02 -2.27845479e-02 -5.36433570e-02\n",
      "  1.90499872e-02 -3.42503153e-02  1.32688358e-02 -5.41317370e-03\n",
      "  7.49740936e-03 -7.36992806e-04 -3.08569502e-02  3.82288396e-02\n",
      " -2.08311658e-02 -3.43154892e-02  5.60239423e-03  1.44999884e-02\n",
      " -3.76364104e-02 -5.11782505e-02 -3.51075418e-02  1.71868168e-02\n",
      "  1.50721967e-02 -9.62026045e-02 -1.53545244e-02  1.58376470e-02\n",
      "  2.42940956e-07 -5.88801643e-03  7.68795684e-02  5.86063601e-02\n",
      "  2.21232846e-02 -2.36690640e-02  5.25274500e-02  1.48661379e-02\n",
      "  7.34177465e-03 -4.98918630e-03  4.37413715e-02 -1.28331631e-02\n",
      "  3.37342508e-02 -1.10814767e-02 -1.33937802e-02 -7.80063868e-02\n",
      " -1.36330724e-02  1.94748845e-02  1.91748294e-03 -3.00251562e-02\n",
      "  1.02674647e-04  9.54533368e-02  1.19653948e-01  3.73371691e-02\n",
      "  4.25127381e-03  2.05129832e-02 -3.85414883e-02 -1.90614145e-02\n",
      "  5.88793233e-02  6.81264624e-02 -3.12595740e-02 -6.50440753e-02\n",
      "  2.48044860e-02  3.90066882e-04  7.54762143e-02 -3.46076041e-02\n",
      "  1.32949306e-02  4.14005741e-02  3.07569485e-02  5.50353108e-03\n",
      " -1.53087522e-03  2.75993031e-02  6.46027224e-03  1.05398074e-02\n",
      " -3.09298802e-02  4.60232496e-02 -3.64921615e-02 -1.39540462e-02\n",
      " -3.53721008e-02  7.97835877e-04  1.40632903e-02  1.80259049e-02\n",
      " -1.43368701e-02  2.19210307e-03 -3.96873318e-02 -1.17281815e-02\n",
      " -4.45220470e-02  8.05770140e-03 -4.04861607e-02  3.56149152e-02\n",
      "  5.12852594e-02 -6.64038733e-02 -5.32594696e-02  8.92899465e-03\n",
      "  1.56424697e-02  1.02110937e-01  8.10770225e-03 -4.03858162e-03\n",
      "  2.02352605e-34 -1.38294008e-02 -1.17623443e-02  1.51006198e-02\n",
      "  8.25896412e-02  2.39228476e-02 -1.10378154e-02  3.65654146e-03\n",
      " -7.44785229e-03  2.94554941e-02  3.52994865e-03 -6.10421970e-02]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=\"cuda\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
    "\n",
    "# Create a list of sentences to turn into numbers\n",
    "sentences = [\n",
    "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
    "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
    "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
    "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
    "]\n",
    "\n",
    "# Sentences are encoded/embedded by calling model.encode()\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "embeddings_dict = dict(zip(sentences, embeddings))\n",
    "\n",
    "# See the embeddings\n",
    "for sentence, embedding in embeddings_dict.items():\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 0,\n",
       "  'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model',\n",
       "  'sentence_chunk': '1. Introduction of Word2vecWord2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space.',\n",
       "  'chunk_char_count': 468,\n",
       "  'chunk_word_count': 75,\n",
       "  'chunk_token_count': 117.0},\n",
       " {'ID': 0,\n",
       "  'Title': 'A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model',\n",
       "  'sentence_chunk': 'For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another. There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit.',\n",
       "  'chunk_char_count': 670,\n",
       "  'chunk_word_count': 112,\n",
       "  'chunk_token_count': 167.5}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "records = df.to_dict(orient=\"records\")\n",
    "records[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to the GPU\n",
    "embedding_model.to(\"cuda\") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090\n",
    "\n",
    "# Create embeddings one by one on the GPU\n",
    "for item in records:\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ID': 1332,\n",
       "  'Title': 'Machine Learning Cheat Sheet — Data Processing Techniques',\n",
       "  'sentence_chunk': 'Disadvantage:Data normalization is sensitive to outliers. One-hot EncodingConvert categorical data into binary variables. For example, convert feature gender into two columns, male and female, with value 0 or 1. Imbalanced Data SetData is not well distributed among different classes. For example, only 0.1% of the transactions are fraud.',\n",
       "  'chunk_char_count': 338,\n",
       "  'chunk_word_count': 48,\n",
       "  'chunk_token_count': 84.5,\n",
       "  'embedding': array([-5.79277426e-02,  2.55570207e-02,  1.96328363e-03, -4.97448109e-02,\n",
       "          1.17675355e-02,  7.16103911e-02,  6.12693615e-02, -1.25502900e-03,\n",
       "         -5.49931414e-02, -9.77407489e-03,  3.99435274e-02,  2.05792878e-02,\n",
       "          1.81785710e-02,  9.15679783e-02, -1.13642281e-02,  1.75265188e-03,\n",
       "          1.67607684e-02, -9.53212287e-03, -2.00027451e-02,  1.79839488e-02,\n",
       "          9.84726287e-03, -4.12900113e-02,  5.32371663e-02,  2.15621386e-02,\n",
       "         -2.12750714e-02,  9.35592875e-03,  1.01062655e-02,  1.27185667e-02,\n",
       "         -3.91932279e-02, -2.33157724e-03,  7.59034883e-03,  5.75873367e-02,\n",
       "          2.44741775e-02, -4.06700224e-02,  1.76372316e-06, -8.50127172e-03,\n",
       "         -4.09286469e-02,  7.16003263e-03, -2.55200192e-02, -1.13823414e-02,\n",
       "         -5.48453592e-02, -6.19005077e-02,  5.70712462e-02,  3.17245871e-02,\n",
       "          9.85554978e-03, -5.91101171e-03,  2.72148363e-02,  3.77173796e-02,\n",
       "         -5.04858941e-02, -3.37683819e-02, -5.37385326e-03, -1.86246969e-02,\n",
       "         -5.53058423e-02,  8.95059016e-03, -1.80178527e-02,  5.91219664e-02,\n",
       "         -1.90213621e-02, -1.24203563e-02, -8.60451832e-02,  1.91793237e-02,\n",
       "         -4.07298133e-02,  2.56386362e-02, -1.02586225e-02, -4.49850149e-02,\n",
       "          1.19272489e-02,  4.51527769e-03,  2.80961804e-02, -5.16560581e-03,\n",
       "          3.51763442e-02, -1.04345214e-02, -3.54508101e-03,  2.26459783e-02,\n",
       "         -5.87708764e-02, -1.26477676e-02,  4.70999591e-02, -3.96187082e-02,\n",
       "         -1.43031701e-02,  1.53319109e-02,  7.48240249e-03,  5.13824187e-02,\n",
       "         -2.94229556e-02,  5.08749001e-02, -8.23213160e-03, -4.28454615e-02,\n",
       "          1.76421702e-02, -4.47678231e-02,  8.32031583e-06, -2.16325391e-02,\n",
       "          1.55199356e-02,  2.28609331e-03,  3.91924195e-02,  8.13167356e-03,\n",
       "          2.39596404e-02, -3.66474651e-02, -3.87202837e-02, -1.34561425e-02,\n",
       "         -7.32040172e-03,  8.12400598e-04,  6.57150671e-02, -8.29648972e-02,\n",
       "          1.58318449e-02,  5.95206097e-02, -7.56409625e-03, -1.86055526e-02,\n",
       "          3.24861892e-02, -1.77714080e-02,  1.90865882e-02,  4.30445708e-02,\n",
       "         -3.74363624e-02, -3.96858379e-02,  2.17275787e-02, -1.19863516e-02,\n",
       "         -3.25467512e-02,  6.54358864e-02,  8.56107846e-03,  7.27251247e-02,\n",
       "         -2.98708081e-02, -3.68943289e-02, -3.67489569e-02, -4.62072454e-02,\n",
       "          1.79397650e-02,  7.79054640e-03,  6.58294186e-03,  2.69224998e-02,\n",
       "          2.31484491e-02,  2.73174066e-02, -8.32369328e-02,  4.67688916e-03,\n",
       "          3.98259573e-02, -3.89905646e-02, -2.32549472e-04,  1.22099789e-03,\n",
       "          4.29618359e-03,  2.61275651e-04,  1.68083217e-02, -1.24314462e-03,\n",
       "          3.50582823e-02,  5.07084234e-03, -6.92492723e-02,  1.22361602e-02,\n",
       "         -3.64423334e-03,  1.67160314e-02,  4.48113400e-03, -2.08717845e-02,\n",
       "         -1.95981283e-03,  5.90852425e-02, -4.63099740e-02,  1.98642798e-02,\n",
       "         -2.62927152e-02,  7.33275386e-03, -1.14604793e-01, -9.82526131e-03,\n",
       "         -4.50138412e-02,  4.87910844e-02,  2.56602243e-02,  4.12397273e-03,\n",
       "          2.58467626e-02,  6.07508756e-02,  1.03890570e-02, -8.71893764e-02,\n",
       "          1.20399678e-02, -4.44181710e-02,  8.06179922e-03, -9.48655140e-03,\n",
       "          7.62012089e-03,  2.03832556e-02, -1.36506176e-02,  2.34453175e-02,\n",
       "         -2.76562781e-03, -5.97532559e-03,  7.10554346e-02,  4.64319997e-02,\n",
       "         -1.69566981e-02, -4.62135375e-02,  1.75477602e-02,  1.47621997e-03,\n",
       "          1.98252350e-02, -1.94414519e-02, -1.24179117e-01, -2.36494304e-03,\n",
       "         -1.76901231e-04,  3.10660210e-02,  2.77797505e-02,  3.76738347e-02,\n",
       "         -8.31332430e-03, -3.93091515e-02,  5.39645040e-03, -2.67185289e-02,\n",
       "         -5.74132763e-02, -3.63397785e-02, -2.93699522e-02, -9.94900707e-03,\n",
       "          6.43247589e-02, -7.93439057e-03, -1.78932603e-02,  3.72996554e-02,\n",
       "          4.82785292e-02, -1.50346360e-03,  3.72084626e-03, -3.24015133e-02,\n",
       "         -1.31219486e-02, -1.28193628e-02, -7.57415965e-02,  7.48851802e-03,\n",
       "          2.87102186e-03,  2.68622721e-03, -1.44844456e-02, -6.63199052e-02,\n",
       "         -5.54830395e-02, -3.58580239e-02,  6.41151983e-03, -3.34646553e-02,\n",
       "          5.55884885e-03, -1.74854640e-02,  1.00666354e-03,  3.89274210e-02,\n",
       "          2.19769747e-04, -5.42933233e-02, -3.52171287e-02,  2.99080834e-02,\n",
       "         -1.84446406e-02,  4.51805592e-02, -5.05290553e-02, -1.90660991e-02,\n",
       "          1.78493150e-02, -4.83560748e-02,  3.26922461e-02,  4.18597646e-02,\n",
       "         -1.66898780e-02,  2.50722189e-02,  1.73080750e-02, -2.48688143e-02,\n",
       "         -1.25425179e-02, -1.24444794e-02, -7.36362934e-02,  9.02519096e-03,\n",
       "         -2.69509126e-02,  6.62259012e-02,  2.57697646e-02, -3.02136168e-02,\n",
       "         -1.82338115e-02,  1.28247412e-02,  4.78300545e-03,  5.99532854e-03,\n",
       "          5.18683298e-03, -2.14975269e-04, -2.69609727e-02,  5.66476025e-03,\n",
       "          4.50318865e-03, -2.86813099e-02,  3.24041285e-02,  1.83664579e-02,\n",
       "         -3.31863649e-02, -2.14537904e-02,  3.30593362e-02, -3.52537073e-02,\n",
       "         -7.85093158e-02,  7.80362450e-03, -1.89173520e-02,  5.26228314e-03,\n",
       "          3.52944545e-02, -1.32660056e-02,  7.84119591e-03,  2.81985607e-02,\n",
       "         -4.84995730e-02, -1.27712255e-02,  6.95732236e-02,  5.98277152e-03,\n",
       "         -1.73507654e-03,  1.25873331e-02,  2.21984964e-02,  1.13317817e-02,\n",
       "          3.82129252e-02, -1.31746558e-02, -1.48397246e-02, -7.49244094e-02,\n",
       "         -1.98674854e-02,  1.37028247e-02,  2.94220764e-02,  8.63671154e-02,\n",
       "         -5.88416448e-03, -1.88468006e-02,  1.39739383e-02, -3.64640392e-02,\n",
       "          8.52936413e-03,  1.52966883e-02,  3.79655580e-03, -1.82318687e-03,\n",
       "          2.76049692e-03,  4.46167029e-02,  3.51452157e-02,  2.27738405e-03,\n",
       "          1.44992238e-02, -4.25728634e-02, -2.23976951e-02,  1.94208175e-02,\n",
       "          1.79557316e-02,  2.50156759e-03, -8.79282784e-03,  2.52906270e-02,\n",
       "         -7.29148090e-03,  3.35722677e-02,  5.26560247e-02, -4.22713533e-02,\n",
       "          6.30395189e-02, -1.49300806e-02,  3.40498053e-02, -4.39704657e-02,\n",
       "         -1.75719932e-02, -2.59234235e-02, -2.73538064e-02, -4.55700569e-02,\n",
       "          2.24749930e-02,  5.55441948e-03,  9.61869769e-03, -7.37901661e-04,\n",
       "         -9.60231274e-02,  5.74629521e-03,  6.02308521e-03,  3.42352465e-02,\n",
       "          1.18203433e-02,  2.19054539e-02, -1.57103073e-02,  2.04193629e-02,\n",
       "          8.54280405e-03, -2.38708816e-02, -2.35086754e-02, -2.00736839e-02,\n",
       "         -2.94220336e-02, -4.22522798e-02,  9.34101827e-03,  1.31669454e-03,\n",
       "         -2.44110208e-02, -2.09255293e-02, -5.25213033e-02,  4.71697561e-02,\n",
       "         -1.00850463e-02,  3.24868299e-02,  5.38606904e-02, -5.37632927e-02,\n",
       "         -8.37343484e-02, -1.33930985e-03,  1.67866573e-02, -2.33504418e-02,\n",
       "          8.39508697e-02,  8.28338601e-03,  3.99647616e-02,  8.34052917e-03,\n",
       "         -1.11754220e-02,  1.14373891e-02, -1.16531365e-02,  4.98755947e-02,\n",
       "         -2.44731884e-02, -4.78593213e-03,  5.15402183e-02,  1.63256414e-02,\n",
       "         -4.54477295e-02, -5.67604962e-04, -9.13141295e-03,  1.76961012e-02,\n",
       "          3.17867249e-02,  3.17982100e-02, -5.99223375e-02, -2.99556982e-02,\n",
       "          6.78039622e-03,  2.52366178e-02, -2.63787191e-02,  1.05061950e-02,\n",
       "         -3.43818404e-02, -3.67280394e-02, -3.01203486e-02, -3.31123509e-02,\n",
       "          5.51056564e-02, -3.59977433e-03,  1.12313591e-03, -2.90677114e-03,\n",
       "          2.75567956e-02, -3.71764898e-02,  6.82727024e-02,  2.72940975e-02,\n",
       "         -6.30177930e-02, -3.92203964e-02,  5.59497857e-03,  6.37344923e-03,\n",
       "          3.65899317e-02, -5.20240292e-02, -9.57463384e-02,  8.38966593e-02,\n",
       "         -2.78347805e-02,  8.48077517e-03,  1.24072062e-03, -3.07429265e-02,\n",
       "         -1.24695279e-01,  5.10327285e-04, -1.23729277e-02, -3.43774119e-03,\n",
       "          8.20295140e-02,  1.93135701e-02,  1.39165008e-02,  1.55650703e-02,\n",
       "          4.45189960e-02, -4.80811708e-02, -5.75544983e-02, -3.79648693e-02,\n",
       "         -2.53383480e-02,  2.38936618e-02, -4.91353785e-05,  3.48321884e-03,\n",
       "         -5.05652390e-02,  3.72097716e-02, -3.10518276e-02,  7.29345456e-02,\n",
       "         -3.07502747e-02, -6.36647549e-03,  6.13288172e-02,  5.44792563e-02,\n",
       "          5.15504833e-03,  2.02990929e-03,  7.36373651e-04,  2.09016930e-02,\n",
       "          1.37880975e-02, -4.11652476e-02,  2.22367086e-02,  1.12916157e-02,\n",
       "         -4.08524424e-02, -3.04472204e-02, -6.42046928e-02,  3.81317511e-02,\n",
       "          4.93001565e-02,  4.06169705e-02, -2.58390717e-02, -1.82189047e-02,\n",
       "         -2.17054095e-02, -4.36226614e-02,  4.40124385e-02,  2.82321014e-02,\n",
       "          7.79650286e-02,  4.08612527e-02, -1.01703554e-01,  7.85030832e-04,\n",
       "         -4.45279814e-02,  2.62445565e-02,  3.36711332e-02,  1.20860748e-02,\n",
       "          1.13304926e-03, -8.11963454e-02,  2.29251124e-02, -3.66576621e-03,\n",
       "          4.61193081e-03,  6.91729737e-03, -4.09512129e-03,  3.44452150e-02,\n",
       "          1.23376865e-02,  9.05077253e-03,  3.38474289e-02,  1.59763712e-02,\n",
       "         -3.98398563e-02,  3.15156132e-02,  7.21083814e-03,  2.26471592e-02,\n",
       "         -5.97437938e-05,  3.09356209e-02, -1.60630606e-02,  1.38897467e-02,\n",
       "         -5.56800179e-02, -3.05997804e-02,  8.98634717e-02, -1.14586968e-02,\n",
       "          8.07427987e-03, -1.56023391e-02, -2.07377579e-02, -8.41961149e-03,\n",
       "         -2.40704529e-02,  1.63706038e-02, -3.64890415e-03, -1.88257340e-02,\n",
       "         -5.84482178e-02,  1.98896453e-02,  2.57268697e-02, -1.01992034e-03,\n",
       "          7.30727986e-02, -6.74204854e-03,  2.47113239e-02,  3.76263037e-02,\n",
       "          1.72472820e-02,  2.17734352e-02, -5.88098317e-02, -1.07175473e-03,\n",
       "         -5.49686700e-03, -1.09492019e-02,  3.45848538e-02,  8.15370306e-02,\n",
       "         -3.40260528e-02,  4.52925265e-02,  1.58020854e-02, -7.39812627e-02,\n",
       "          1.31923966e-02,  7.55403563e-02, -2.21364852e-02,  4.63549681e-02,\n",
       "          3.61350528e-03, -6.98798569e-03, -6.07335381e-02,  6.90461649e-03,\n",
       "          5.80765028e-03,  2.25621611e-02, -2.98169125e-02,  5.91206597e-03,\n",
       "          7.09868222e-03,  6.95316419e-02, -1.88026763e-02,  2.59289052e-02,\n",
       "         -6.33568177e-03,  2.48288680e-02,  2.31598387e-03, -3.62202972e-02,\n",
       "         -8.56815279e-02, -2.79479148e-03, -1.42821511e-02, -1.82413533e-02,\n",
       "         -1.40494388e-02, -3.93116474e-03,  1.76121425e-02, -7.94546027e-03,\n",
       "          3.34590152e-02,  4.81600314e-02, -1.91683490e-02, -3.50998156e-02,\n",
       "         -4.44796607e-02,  4.79112826e-02,  2.21588500e-02, -3.89632210e-02,\n",
       "         -7.24456608e-02, -1.82787143e-02,  2.56745648e-02,  4.14642356e-02,\n",
       "          8.73126015e-02, -3.54049169e-02,  1.26434863e-02, -4.06553671e-02,\n",
       "         -4.96977605e-02, -4.53046858e-02, -3.29442620e-02,  4.72085550e-04,\n",
       "          6.62143901e-02,  7.21444283e-03,  7.10866740e-03, -2.02624444e-02,\n",
       "          2.29940820e-03, -5.58130734e-04,  3.79913650e-03, -1.51615450e-02,\n",
       "         -5.38117019e-03,  1.50832934e-02, -4.04833350e-03, -5.38949025e-33,\n",
       "         -2.75363084e-02, -2.29794905e-02, -4.51857522e-02, -7.42034847e-03,\n",
       "          1.08353766e-02, -4.29089405e-02,  1.19471867e-02,  2.47488525e-02,\n",
       "         -2.12586764e-02, -7.53636379e-03, -4.08181809e-02,  1.37987901e-02,\n",
       "         -3.62173957e-03,  1.50939198e-02,  4.41102311e-02, -2.76754051e-02,\n",
       "          3.14387009e-02, -5.37790209e-02, -2.63248608e-02, -2.30352697e-03,\n",
       "          4.57501272e-03, -4.02067266e-02,  1.24896811e-02, -5.19505441e-02,\n",
       "         -3.28051969e-02, -1.83390956e-02, -9.89000127e-03,  2.69732773e-02,\n",
       "          1.80115849e-02, -4.42345813e-02,  3.83494347e-02,  1.76068228e-02,\n",
       "          2.46622153e-02, -2.19446719e-02, -5.75801497e-03,  2.82927379e-02,\n",
       "         -5.20571955e-02,  1.92318372e-02, -1.20689059e-02,  6.23099245e-02,\n",
       "          8.19196254e-02, -4.97127734e-02,  4.39951979e-02,  2.49792002e-02,\n",
       "         -4.17793393e-02, -4.03379498e-04, -5.29931299e-02, -3.24018225e-02,\n",
       "         -2.58162674e-02,  6.64137527e-02, -2.07126383e-02, -3.91329610e-04,\n",
       "         -8.23266059e-03,  7.88621381e-02,  3.17116547e-03,  5.14161848e-02,\n",
       "          1.59284845e-03, -2.50031110e-02, -2.13412177e-02, -1.27264354e-02,\n",
       "          9.65721086e-02, -1.98777877e-02,  5.98054985e-03, -8.65326636e-03,\n",
       "         -9.39669181e-03, -2.04195008e-02,  4.73476909e-02, -8.20003357e-03,\n",
       "         -3.07737384e-02,  1.36811081e-02,  4.34772484e-03, -3.89576331e-02,\n",
       "          2.63168328e-02,  6.41579255e-02,  1.00459315e-01, -6.39110878e-02,\n",
       "         -7.90232942e-02,  4.22317982e-02, -3.93724628e-02, -8.57187808e-02,\n",
       "          1.02098659e-02,  2.87285987e-02, -2.03160867e-02, -7.48649240e-02,\n",
       "          2.60203611e-02, -1.24543365e-02,  1.80810913e-02, -2.62488835e-02,\n",
       "          7.23050209e-03, -3.04606948e-02,  2.19753813e-02,  9.44973826e-02,\n",
       "          9.11782030e-03,  4.90837218e-03, -6.67969836e-03,  7.32343793e-02,\n",
       "          2.60393638e-02,  1.40977325e-02, -9.72262491e-03, -5.74328229e-02,\n",
       "         -1.98050179e-02,  1.63872726e-02,  2.00296175e-02,  4.22501117e-02,\n",
       "          1.72081392e-03,  3.26296538e-02,  6.99092969e-02,  1.27636865e-02,\n",
       "         -5.98369204e-02, -1.25240432e-02,  5.64879142e-02,  2.86477208e-02,\n",
       "          4.14094888e-02,  5.86792007e-02, -2.38305144e-02, -9.06613544e-02,\n",
       "          1.58627834e-02,  2.16760626e-03,  3.50434445e-02,  3.16648884e-03,\n",
       "         -6.35119230e-02,  2.97598056e-02,  3.37927230e-02, -5.87447919e-02,\n",
       "          6.29529953e-02,  1.79077387e-02,  1.24032209e-02, -7.11374059e-02,\n",
       "          5.17125130e-02, -3.96389961e-02, -1.68968346e-02, -3.36236432e-02,\n",
       "          2.47574775e-07,  3.34722102e-02,  5.87557501e-04,  3.21050435e-02,\n",
       "         -1.77454110e-02, -1.75804235e-02,  4.93399538e-02, -2.69272849e-02,\n",
       "          4.19370048e-02,  5.19508310e-02, -1.35119446e-02, -6.41392963e-03,\n",
       "          4.13718447e-03, -2.94192247e-02,  7.07886145e-02, -9.85904844e-05,\n",
       "         -5.15966378e-02,  2.03891378e-02, -1.97864827e-02,  2.51001827e-02,\n",
       "          2.72490326e-02,  2.22641174e-02,  5.83225116e-02, -4.62872051e-02,\n",
       "         -1.34865679e-02,  6.06748611e-02, -4.88380762e-03, -1.77606363e-02,\n",
       "          4.34104726e-03,  4.34427448e-02, -1.49008399e-02,  5.30779138e-02,\n",
       "          2.95655783e-02,  2.41909418e-02,  4.89428267e-02,  1.85241625e-02,\n",
       "          5.93656022e-03, -5.24835195e-04,  8.15038197e-03, -6.06781952e-02,\n",
       "         -1.14532281e-02, -1.44253988e-02, -6.08593002e-02, -1.21293962e-02,\n",
       "         -3.12800892e-02,  4.98212390e-02,  5.93924150e-03, -4.28141467e-02,\n",
       "         -2.11218949e-02,  2.59222481e-02,  4.35649082e-02,  6.04543723e-02,\n",
       "          2.42554061e-02, -2.60285698e-02,  9.40918922e-03,  2.15682480e-02,\n",
       "         -1.72298104e-02,  2.07046978e-02, -2.03634780e-02,  5.76051418e-03,\n",
       "          4.33341414e-02,  1.98084731e-02, -3.55300978e-02,  4.56635058e-02,\n",
       "          6.32076114e-02,  5.40261604e-02,  1.12999948e-02, -1.21961543e-02,\n",
       "          2.91033368e-34,  1.76945198e-02,  3.34348865e-02,  5.76206706e-02,\n",
       "         -2.11988725e-02, -3.72818974e-03, -5.79703189e-02,  2.95814071e-02,\n",
       "         -4.22703847e-02, -9.38603841e-03,  1.61568969e-02, -1.65402684e-02],\n",
       "        dtype=float32)}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(records, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings to file\n",
    "chunks_and_embeddings_df = pd.DataFrame(records)\n",
    "embeddings_df_save_path = \"./data/chunks_and_embeddings_df.csv\"\n",
    "chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>1. Introduction of Word2vecWord2vec is one of ...</td>\n",
       "      <td>468</td>\n",
       "      <td>75</td>\n",
       "      <td>117.00</td>\n",
       "      <td>[ 1.14987101e-02  4.61726263e-02 -7.97047652e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>For instance, the words women, men, and human ...</td>\n",
       "      <td>670</td>\n",
       "      <td>112</td>\n",
       "      <td>167.50</td>\n",
       "      <td>[ 5.78960031e-02 -1.32116340e-02  4.08314820e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>For more details about the word2vec algorithm,...</td>\n",
       "      <td>601</td>\n",
       "      <td>96</td>\n",
       "      <td>150.25</td>\n",
       "      <td>[ 3.93238813e-02  2.01313253e-02  1.00309318e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>Gensim depends on the following software:Pytho...</td>\n",
       "      <td>774</td>\n",
       "      <td>119</td>\n",
       "      <td>193.50</td>\n",
       "      <td>[ 7.78792519e-03  3.13875079e-02 -2.66925362e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "      <td>We will use these features to generate the wor...</td>\n",
       "      <td>725</td>\n",
       "      <td>120</td>\n",
       "      <td>181.25</td>\n",
       "      <td>[ 9.57979169e-03  1.54464487e-02 -1.20048057e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              Title  \\\n",
       "0   0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "1   0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "2   0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "3   0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "4   0  A Beginner’s Guide to Word Embedding with Gens...   \n",
       "\n",
       "                                      sentence_chunk  chunk_char_count  \\\n",
       "0  1. Introduction of Word2vecWord2vec is one of ...               468   \n",
       "1  For instance, the words women, men, and human ...               670   \n",
       "2  For more details about the word2vec algorithm,...               601   \n",
       "3  Gensim depends on the following software:Pytho...               774   \n",
       "4  We will use these features to generate the wor...               725   \n",
       "\n",
       "   chunk_word_count  chunk_token_count  \\\n",
       "0                75             117.00   \n",
       "1               112             167.50   \n",
       "2                96             150.25   \n",
       "3               119             193.50   \n",
       "4               120             181.25   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 1.14987101e-02  4.61726263e-02 -7.97047652e-...  \n",
       "1  [ 5.78960031e-02 -1.32116340e-02  4.08314820e-...  \n",
       "2  [ 3.93238813e-02  2.01313253e-02  1.00309318e-...  \n",
       "3  [ 7.78792519e-03  3.13875079e-02 -2.66925362e-...  \n",
       "4  [ 9.57979169e-03  1.54464487e-02 -1.20048057e-...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import saved file and view\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.set_index(\"ID\")\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11856, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hnswlib\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"./data/chunks_and_embeddings_df.csv\")\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = np.array(text_chunks_and_embedding_df[\"embedding\"].tolist())\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the HNSW index\n",
    "dim = embeddings.shape[1]  # Dimensionality of the vectors\n",
    "\n",
    "# Initializing an HNSW index\n",
    "p = hnswlib.Index(space='cosine', dim=dim)  # or use 'cosine' if more appropriate\n",
    "\n",
    "# Initialize the index\n",
    "# Specify the maximum number of elements in the index\n",
    "# Parameters can be adjusted based on dataset and requirements\n",
    "p.init_index(max_elements=embeddings.shape[0], ef_construction=200, M=16)\n",
    "\n",
    "# Add items to the index\n",
    "# Here, we don't specify the ids, so HNSWLIB will generate them automatically\n",
    "p.add_items(embeddings)\n",
    "\n",
    "# Optional: Set ef parameter for controlling query time/accuracy trade-off\n",
    "p.set_ef(50)  # Setting it higher leads to more accurate but slower searches\n",
    "\n",
    "# Now, your index is ready for querying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: price prediction model\n"
     ]
    }
   ],
   "source": [
    "query = \"price prediction model\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples \n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query)\n",
    "\n",
    "# # 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "# from time import perf_counter as timer\n",
    "\n",
    "# start_time = timer()\n",
    "# dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "# end_time = timer()\n",
    "\n",
    "# print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# # 4. Get the top-k results (we'll keep this to 5)\n",
    "# top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "# top_results_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbor ids: [[10871 11770  3020  5589 11771 11663  5585 11590 10624 11649]]\n",
      "Distances: [[0.31617194 0.3314786  0.39219975 0.40445375 0.41107804 0.4178326\n",
      "  0.42525178 0.42620337 0.42673832 0.4293744 ]]\n"
     ]
    }
   ],
   "source": [
    "# Searching for the 5 nearest neighbors\n",
    "ids, distances = p.knn_query(query_embedding, k = 5)\n",
    "\n",
    "print(\"Nearest neighbor ids:\", ids)\n",
    "print(\"Distances:\", distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text \n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'price prediction model'\n",
      "\n",
      "Results:\n",
      "Score: 0.31617194414138794\n",
      "Title:\n",
      "Tools/Tips Critical to Any Machine Learning Project\n",
      "Text:\n",
      "Although we found that we can predict the future prices with some accuracy, it’s\n",
      "still not fit to capture the special cases (sudden price change based on other\n",
      "market factors). The purpose here was not to solve the problem completely\n",
      "(although we can if we figure a way to integrate other factors to our problem\n",
      "statement), but to realise, what we can do with Machine Learning, seating at our\n",
      "homes, with our mediocre laptops. That’s the power of Machine Learning. Keep\n",
      "coding.\n",
      "ID: 1263\n",
      "\n",
      "\n",
      "Score: 0.3314785957336426\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "We will try to predict the future prices of Bitcoin by using its closing_price\n",
      "feature. What Model to Use?To perform forecasting, we will need a machine\n",
      "learning model. Most people think of multi-linear regression when they want to\n",
      "predict values. But for Time-series data, this is not a good idea. The main\n",
      "reason to not opt for regression for Time-Series Data is we are interested in\n",
      "predicting the future, which would be extrapolation (predicting outside the\n",
      "range of the data) for linear regression.\n",
      "ID: 1376\n",
      "\n",
      "\n",
      "Score: 0.3921997547149658\n",
      "Title:\n",
      "How Microsoft Azure Machine Learning Studio Clarifies Data Science\n",
      "Text:\n",
      "We’ll predict the price in far-right column (column 26, titled “price”) using\n",
      "the variables for a specific automobile. Note the histograms of each column that\n",
      "are given and the details on the distribution of the data in the right pane.\n",
      "This quick look seemed more time-consuming to find in other tools I have used.\n",
      "Prepare the dataAs any experienced data scientists knows, datasets usually\n",
      "require some preprocessing before they can be analyzed. In this case, there are\n",
      "missing values present in the columns of various rows.\n",
      "ID: 350\n",
      "\n",
      "\n",
      "Score: 0.40445375442504883\n",
      "Title:\n",
      "Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III)\n",
      "Text:\n",
      "Time Series forecasting. There are many approaches that you can use for this\n",
      "purpose. But we’ll build a Deep Neural Network that does some forecasting for us\n",
      "and use it to predict future Bitcoin price. ModelingAll models we’ve built so\n",
      "far do not allow for operating on sequence data. Fortunately, we can use a\n",
      "special class of Neural Network models known as Recurrent Neural Networks (RNNs)\n",
      "just for this purpose.\n",
      "ID: 682\n",
      "\n",
      "\n",
      "Score: 0.4110780358314514\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "And as we know that in linear regression any sort of extrapolation is not\n",
      "advisable. For time-series data, it is better to use the Auto Regressive\n",
      "Integrated Moving Average, or ARIMA Models. ARIMAARIMA is actually a class of\n",
      "models that ‘explains’ a given time series based on its own past values, that\n",
      "is, its own lags and the lagged forecast errors, so that equation can be used to\n",
      "forecast future values. Any ‘non-seasonal’ time series that exhibits patterns\n",
      "and is not a random white noise can be modeled with ARIMA models. The hypothesis\n",
      "testing performed as discussed below, shows the prices were not seasonal, hence\n",
      "we can use an ARIMA model.\n",
      "ID: 1376\n",
      "\n",
      "\n",
      "Score: 0.417832612991333\n",
      "Title:\n",
      "Stock Market Prediction for the Australian Securities Exchange (ASX) With Deep\n",
      "Learning: Using Technical Indicators and a Long Short-Term Memory (LSTM) Model\n",
      "Text:\n",
      "Lecture Notes in Computer Science.2007;\n",
      "https://link.springer.com/chapter/10.1007/978-3-540-72395-0_132[3] Hegazy O,\n",
      "Soliman OS, Salam MA. A Machine Learning Model for Stock Market Prediction.\n",
      "International Journal of Computer Science and Telecommunications.2013; 17–23.\n",
      "ID: 1357\n",
      "\n",
      "\n",
      "Score: 0.4252517819404602\n",
      "Title:\n",
      "Cryptocurrency price prediction using LSTMs | TensorFlow for Hackers (Part III)\n",
      "Text:\n",
      "Here, we’ll have a look at how you might build a model to help you along the\n",
      "crazy journey. Or you might be having money problems?Here is one possible\n",
      "solution:Here is the plan:Cryptocurrency data overview Time Series Data\n",
      "preprocessing Build and train LSTM model in TensorFlow 2 Use the model to\n",
      "predict future Bitcoin priceData OverviewOur dataset comes from Yahoo!Finance\n",
      "and covers all available (at the time of this writing) data on Bitcoin-USD\n",
      "price. Let’s load it into a Pandas dataframe:Note that we sort the data by Date\n",
      "just in case.\n",
      "ID: 682\n",
      "\n",
      "\n",
      "Score: 0.4262033700942993\n",
      "Title:\n",
      "Performing a Time Series Analysis on the AAPL Stock Index.\n",
      "Text:\n",
      "Time series data can be used for forecasting. Examples of time series data\n",
      "include; stock prices, temperature over time, heights of ocean tides, and so on.\n",
      "We will focus our attention on forecasting stock prices using time series\n",
      "analysis. Forecasting stock prices is a very difficult and challenging task in\n",
      "the financial market because the trends of stock prices are non-linear and non-\n",
      "stationary time-series data. The main idea of this article is to predict the\n",
      "future stock prices of a particular company to help us when investing.\n",
      "ID: 1350\n",
      "\n",
      "\n",
      "Score: 0.42673832178115845\n",
      "Title:\n",
      "Predicting Stock Price with LSTM\n",
      "Text:\n",
      "I have used Sigmoid activation for last layer which may suffer from limitation\n",
      "of not being able to predict a price greater than ‘max’ price in dataset. You\n",
      "could try ‘Linear’ activation for last layer to solve this. Fixed a typo in\n",
      "“converting data to time-series” section. Thanks to the readers for bringing\n",
      "these to my attention. UPDATE 21/1/2020As mentioned in some of the comments, I\n",
      "was exploring other ways to approach the stock prediction problem.\n",
      "ID: 1241\n",
      "\n",
      "\n",
      "Score: 0.4293743968009949\n",
      "Title:\n",
      "Stock Market Prediction for the Australian Securities Exchange (ASX) With Deep\n",
      "Learning: Using Technical Indicators and a Long Short-Term Memory (LSTM) Model\n",
      "Text:\n",
      "Long short-term memory (LSTM) modelPreprocessing: Based on the analysis done in\n",
      "the previous section, this section builds an LSTM neural network model for the\n",
      "prediction of the next day stock price moment; whether it is going upward or\n",
      "downward. The training and validation sets will be using during the LSTM network\n",
      "training, while the test set will be used for trading strategy implementation\n",
      "and additional testing of the final model. Next, the data is pre-processed. The\n",
      "data is normalized to be between 0 and 1 using MinMaxScaler. Note that the\n",
      "maximum and minimum is with respect to each ticker in each train data frame.\n",
      "ID: 1357\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(distances[0], ids[0]):\n",
    "    print(f\"Score: {score}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Title:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"Title\"])\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"ID: {pages_and_chunks[idx]['ID']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between vector1 and vector2: tensor(14.)\n",
      "Dot product between vector1 and vector3: tensor(32.)\n",
      "Dot product between vector1 and vector4: tensor(-14.)\n",
      "Cosine similarity between vector1 and vector2: tensor(1.0000)\n",
      "Cosine similarity between vector1 and vector3: tensor(0.9746)\n",
      "Cosine similarity between vector1 and vector4: tensor(-1.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return torch.dot(vector1, vector2)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = torch.dot(vector1, vector2)\n",
    "\n",
    "    # Get Euclidean/L2 norm of each vector (removes the magnitude, keeps direction)\n",
    "    norm_vector1 = torch.sqrt(torch.sum(vector1**2))\n",
    "    norm_vector2 = torch.sqrt(torch.sum(vector2**2))\n",
    "\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "# Example tensors\n",
    "vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)\n",
    "vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "\n",
    "    # Get dot product scores on embeddings\n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "\n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print(\"Title:\")\n",
    "        print_wrapped(pages_and_chunks[idx][\"Title\"])\n",
    "        print(\"Text:\")\n",
    "        print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further (and check the results)\n",
    "        print(f\"ID: {pages_and_chunks[idx]['ID']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 11856 embeddings: 0.00004 seconds.\n"
     ]
    }
   ],
   "source": [
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 11856 embeddings: 0.00003 seconds.\n",
      "Query: price prediction model\n",
      "\n",
      "Results:\n",
      "Score: 0.6838\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "And as we know that in linear regression any sort of extrapolation is not\n",
      "advisable. For time-series data, it is better to use the Auto Regressive\n",
      "Integrated Moving Average, or ARIMA Models. ARIMAARIMA is actually a class of\n",
      "models that ‘explains’ a given time series based on its own past values, that\n",
      "is, its own lags and the lagged forecast errors, so that equation can be used to\n",
      "forecast future values. Any ‘non-seasonal’ time series that exhibits patterns\n",
      "and is not a random white noise can be modeled with ARIMA models. The hypothesis\n",
      "testing performed as discussed below, shows the prices were not seasonal, hence\n",
      "we can use an ARIMA model.\n",
      "ID: 1376\n",
      "\n",
      "\n",
      "Score: 0.6685\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "And as we know that in linear regression any sort of extrapolation is not\n",
      "advisable. For time-series data, it is better to use the Auto Regressive\n",
      "Integrated Moving Average, or ARIMA Models. ARIMAARIMA is actually a class of\n",
      "models that ‘explains’ a given time series based on its own past values, that\n",
      "is, its own lags and the lagged forecast errors, so that equation can be used to\n",
      "forecast future values. Any ‘non-seasonal’ time series that exhibits patterns\n",
      "and is not a random white noise can be modeled with ARIMA models. The hypothesis\n",
      "testing performed as discussed below, shows the prices were not seasonal, hence\n",
      "we can use an ARIMA model.\n",
      "ID: 1376\n",
      "\n",
      "\n",
      "Score: 0.6078\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "And as we know that in linear regression any sort of extrapolation is not\n",
      "advisable. For time-series data, it is better to use the Auto Regressive\n",
      "Integrated Moving Average, or ARIMA Models. ARIMAARIMA is actually a class of\n",
      "models that ‘explains’ a given time series based on its own past values, that\n",
      "is, its own lags and the lagged forecast errors, so that equation can be used to\n",
      "forecast future values. Any ‘non-seasonal’ time series that exhibits patterns\n",
      "and is not a random white noise can be modeled with ARIMA models. The hypothesis\n",
      "testing performed as discussed below, shows the prices were not seasonal, hence\n",
      "we can use an ARIMA model.\n",
      "ID: 1376\n",
      "\n",
      "\n",
      "Score: 0.5955\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "And as we know that in linear regression any sort of extrapolation is not\n",
      "advisable. For time-series data, it is better to use the Auto Regressive\n",
      "Integrated Moving Average, or ARIMA Models. ARIMAARIMA is actually a class of\n",
      "models that ‘explains’ a given time series based on its own past values, that\n",
      "is, its own lags and the lagged forecast errors, so that equation can be used to\n",
      "forecast future values. Any ‘non-seasonal’ time series that exhibits patterns\n",
      "and is not a random white noise can be modeled with ARIMA models. The hypothesis\n",
      "testing performed as discussed below, shows the prices were not seasonal, hence\n",
      "we can use an ARIMA model.\n",
      "ID: 1376\n",
      "\n",
      "\n",
      "Score: 0.5889\n",
      "Title:\n",
      "Forecasting Future Prices of Cryptocurrency using Historical Data\n",
      "Text:\n",
      "And as we know that in linear regression any sort of extrapolation is not\n",
      "advisable. For time-series data, it is better to use the Auto Regressive\n",
      "Integrated Moving Average, or ARIMA Models. ARIMAARIMA is actually a class of\n",
      "models that ‘explains’ a given time series based on its own past values, that\n",
      "is, its own lags and the lagged forecast errors, so that equation can be used to\n",
      "forecast future values. Any ‘non-seasonal’ time series that exhibits patterns\n",
      "and is not a random white noise can be modeled with ARIMA models. The hypothesis\n",
      "testing performed as discussed below, shows the prices were not seasonal, hence\n",
      "we can use an ARIMA model.\n",
      "ID: 1376\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
